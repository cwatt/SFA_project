{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: pipeline wip\n",
    "\n",
    "Cassandra Wattenburger, 07/08/21\n",
    "\n",
    "### Notes:\n",
    "* Original script written by Roli Wilhelm, heavily modified by Cassandra Wattenburger\n",
    "* Running script using QIIME2 v2021.4\n",
    "* Incorporating broken pieces back into pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "### Pipeline to process raw sequences into phyloseq object with DADA2 ###\n",
    "* Prep for Import to QIIME2  (Combine two index files)\n",
    "* Import to QIIME2\n",
    "* Demultiplex\n",
    "* Denoise and Merge\n",
    "* Prepare OTU Tables and Rep Sequences  *(Note: sample names starting with a digit will break this step)*\n",
    "* Classify Seqs\n",
    "\n",
    "*100% Appropriated from the \"Atacama Desert Tutorial\" for QIIME2*\n",
    "\n",
    "### Pipeline can handle both 16S rRNA gene and ITS sequences####\n",
    "* Tested on 515f and 806r\n",
    "* Tested on ITS1\n",
    "\n",
    "### Commands to install dependencies ####\n",
    "##### || QIIME2 and biopython ||\n",
    "** Note: QIIME2 is still actively in development, and I've noticed frequent new releases. Check for the most up-to-date conda install file <https://docs.qiime2.org/2017.11/install/native/#install-qiime-2-within-a-conda-environment> and follow the instructions to install QIIME2 in Linux (64-bit).\n",
    "\n",
    "To activate the QIIME2 environment:\n",
    "* source activate qiime2-pipeline\n",
    "\n",
    "After you install the QIIME2 environment, you must also install BioPython into the environment for this script to work due to a custom barcode concatenation script. To install biopython make sure the QIIME2 environment is activated and run:\n",
    "* conda intall -c anaconda biopython\n",
    "\n",
    "##### || Copyrighter rrn database ||\n",
    "* The script will automatically install the curated GreenGenes rrn attribute database\n",
    "* https://github.com/fangly/AmpliCopyrighter\n",
    "\n",
    "### Citations ###\n",
    "* Caporaso, J. G., Kuczynski, J., Stombaugh, J., Bittinger, K., Bushman, F. D., Costello, E. K., *et al.* (2010). QIIME allows analysis of high-throughput community sequencing data. Nature methods, 7(5), 335-336.\n",
    "\n",
    "* Angly, F. E., Dennis, P. G., Skarshewski, A., Vanwonterghem, I., Hugenholtz, P., & Tyson, G. W. (2014). CopyRighter: a rapid tool for improving the accuracy of microbial community profiles through lineage-specific gene copy number correction. Microbiome, 2(1), 11.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: User Input\n",
    "\n",
    "### Metadata requirements\n",
    "* Must be located in the project directory\n",
    "* Must be .tsv format \n",
    "* First column named \"SampleID\" for samples\n",
    "* One column named \"BarcodeSequence\" with the relevant barcode seqeunces (rev. comp reverse concatenated with forward barcode sequence)\n",
    "\n",
    "The output directory will be created inside the project directory when you run the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, numpy as np\n",
    "\n",
    "# Prepare an object with the name of the library and all related file paths\n",
    "# datasets = [['project name', 'project directory path', 'raw data directory', 'custom scripts directory', 'read1 file name', 'read2 file name', 'index1 file name', 'index2 file name', 'metadata file name', 'domain of life'], ...]\n",
    "datasets = [['test', \n",
    "             '/home/cassi/testing', \n",
    "             '/home/backup_files/raw_reads/test.cassi',\n",
    "             '/home/cassi/scripts',\n",
    "             '141178_JPRKL_SFA2_rep1_062421_S1_R1_001.fastq.gz', \n",
    "             '141178_JPRKL_SFA2_rep1_062421_S1_R2_001.fastq.gz', \n",
    "             '141178_JPRKL_SFA2_rep1_062421_S1_I1_001.fastq.gz', \n",
    "             '141178_JPRKL_SFA2_rep1_062421_S1_I2_001.fastq.gz', \n",
    "             'metadata.tsv', 'bacteria']]\n",
    "\n",
    "# Set # of processors (10 ussually good)\n",
    "processors = 10\n",
    "\n",
    "# Which bacterial database will you use? Silva or GreenGenes\n",
    "db = \"Silva\"\n",
    "\n",
    "# Phylogenetic tree (non-fungal data)\n",
    "treepath = \"/home/cassi/testing\" # full path to the phylogenetic tree file to be made\n",
    "treename = \"test\" # name prefix for the tree file\n",
    "\n",
    "## Enter minimum support for keeping QIIME classification\n",
    "# Note: Classifications that do not meet this criteria will simply be retained, but labeled 'putative'\n",
    "# Note: I typically use 0.9\n",
    "min_support = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Truncate sequence identifiers\n",
    "\n",
    "This step removes a portion at the end of the sequence ID that is incompatible with QIIME2. It will also create a modified directory in your raw data directory to house the modified data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: can't read /home/backup_files/raw_reads/test.cassi/modified/141178_JPRKL_SFA2_rep1_062421_S1_I2_001fastq: No such file or directory\n",
      "rm: cannot remove '/home/backup_files/raw_reads/test.cassi/modified/141178_JPRKL_SFA2_rep1_062421_S1_I2_001fastq': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "### SLOW STEP (decompressing and recompressing takes a bit of time)\n",
    "\n",
    "for dataset in datasets:\n",
    "    raw = dataset[2]\n",
    "    read1 = dataset[4]\n",
    "    read2 = dataset[5]\n",
    "    index1 = dataset[6]\n",
    "    index2 = dataset[7]\n",
    "    \n",
    "    # Create a directory to place modified sequence data\n",
    "    if not os.path.isdir(os.path.join(raw, \"modified\")):\n",
    "        !mkdir $raw/modified\n",
    "    \n",
    "    # Copy/paste all raw sequence data to modified directory\n",
    "    !cp $raw/*.fastq.gz $raw/modified\n",
    "    \n",
    "    # Decompress all files\n",
    "    !unpigz $raw/modified/*.fastq.gz\n",
    "    \n",
    "    # Decompressed file names\n",
    "    read1decomp = re.sub(\".fastq.gz\", \".fastq\", read1)\n",
    "    read2decomp = re.sub(\".fastq.gz\", \".fastq\", read2)\n",
    "    index1decomp = re.sub(\".fastq.gz\", \".fastq\", index1)\n",
    "    index2decomp = re.sub(\".fastq.gz\", \".fastq\", index2)\n",
    "    \n",
    "    # Remove problematic part of sequence IDs\n",
    "    !sed 's/\\ [0-9]:[YN]:[0-9]:[0-9]$//g' $raw/modified/$read1decomp > $raw/modified/read1_mod.fastq\n",
    "    !sed 's/\\ [0-9]:[YN]:[0-9]:[0-9]$//g' $raw/modified/$read2decomp > $raw/modified/read2_mod.fastq\n",
    "    !sed 's/\\ [0-9]:[YN]:[0-9]:[0-9]$//g' $raw/modified/$index1decomp > $raw/modified/index1_mod.fastq\n",
    "    !sed 's/\\ [0-9]:[YN]:[0-9]:[0-9]$//g' $raw/modified/$index2decomp > $raw/modified/index2_mod.fastq\n",
    "    \n",
    "    # Delete copied files\n",
    "    !rm $raw/modified/$read1decomp\n",
    "    !rm $raw/modified/$read2decomp\n",
    "    !rm $raw/modified/$index1decomp\n",
    "    !rm $raw/modified/$index2decomp\n",
    "    \n",
    "    # Recompress modified read files\n",
    "    !pigz $raw/modified/read1_mod.fastq\n",
    "    !pigz $raw/modified/read2_mod.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Concatenate barcodes\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Done on command line.\n",
    "\n",
    "1. Run 'concatenate_barcodes_mod.py' on the modified index1 and index2 files\n",
    "1. Recompress all files to .fastq.gz (inlcuding the barcodes.fastq file that you just created)\n",
    "\n",
    "Will work on incorporating this directly into pipeline in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Move data to output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    raw = dataset[3]\n",
    "    read1 = dataset[4]\n",
    "    read2 = dataset[5]\n",
    "    \n",
    "    # Create output directory if it doesn't exist already\n",
    "    if not os.path.isdir(os.path.join(directory, output)):\n",
    "        !mkdir $directory/$output\n",
    "    \n",
    "    # Create a symbolic link to the read data (files too big, copy/paste a waste of space)\n",
    "    # QIIME2 import requires a directory containing files named: forward.fastq.gz, reverse.fastq.gz and barcodes.fastq.gz \n",
    "    !ln -s $raw/$read1 $directory/$output/forward.fastq.gz\n",
    "    !ln -s $raw/$read2 $directory/$output/reverse.fastq.gz\n",
    "    \n",
    "    # Move concatenated barcodes to project directory\n",
    "    !cp $raw/barcodes.fastq.gz $directory/$output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Import into QIIME2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime tools import\",\n",
    "        \"--type EMPPairedEndSequences\",\n",
    "        \"--input-path \"+directory+\"/\"+output,\n",
    "        \"--output-path \"+directory+\"/\"+output+\"/\"+name+\".qza\"\n",
    "    ]))\n",
    "    \n",
    "    # This more direct command is broken by the fact QIIME uses multiple dashes in their arguments (is my theory)\n",
    "    #!qiime tools import --type EMPPairedEndSequences --input-path $directory/output --output-path $directory/output/$name.qza\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Demultiplex\n",
    "\n",
    "### Notes\n",
    "* The barcode you supply to QIIME is now a concatenation of your forward and reverse barcode\n",
    "* Your 'forward' barcode is actually the reverse complement of your reverse barcode and the 'reverse' is your forward barcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SLOW STEP\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    metadata = dataset[8]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime demux emp-paired\",\n",
    "        \"--m-barcodes-file \"+directory+\"/\"+metadata,\n",
    "        \"--m-barcodes-column BarcodeSequence\",\n",
    "        \"--p-no-golay-error-correction\",\n",
    "        \"--i-seqs \"+directory+\"/\"+output+\"/\"+name+\".qza\",\n",
    "        \"--o-per-sample-sequences \"+directory+\"/\"+output+\"/\"+name+\".demux\",\n",
    "        \"--o-error-correction-details \"+directory+\"/\"+output+\"/\"+name+\".demux-details.qza\"\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Demultiplexing is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Visualize quality scores\n",
    "\n",
    "Drop output from below command into https://view.qiime2.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime demux summarize\",\n",
    "        \"--i-data \"+directory+\"/\"+output+\"/\"+name+\".demux.qza\",\n",
    "        \"--o-visualization \"+directory+\"/\"+output+\"/\"+name+\".demux.qzv\"\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Trimming parameters | USER INPUT REQUIRED\n",
    "\n",
    "Based on the quality scores of the bp along the reads, choose trim and truncate values. Trim refers to the start of a sequence and truncate the total length (i.e. number of bases to remove from end).\n",
    "\n",
    "All trimming parameters must be the same for datasets that will be directly compared to one-another because ASVs are determined by sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## User Input Required\n",
    "\n",
    "## Input your trimming parameters into a python dictionary for all libraries\n",
    "# trim_dict = {\"LibraryName1\":[trim_forward, truncate_forward, trim_reverse, truncate_reverse],\n",
    "#              \"LibraryName2\":[trim_forward, truncate_forward, trim_reverse, truncate_reverse],\n",
    "#               etc...}\n",
    "\n",
    "# The example in the Atacama Desert Tutorial trims 13 bp from the start of each read and does not remove any bases from the end of the 150 bp reads:\n",
    "#  --p-trim-left-f 13 \\  \n",
    "#  --p-trim-left-r 13 \\\n",
    "#  --p-trunc-len-f 150 \\\n",
    "#  --p-trunc-len-r 150\n",
    "\n",
    "trim_dict = {\"SFA2rep1trial\":[10,240,10,220]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Trim, denoise and join (aka 'merge') reads using DADA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SLOW STEP\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime dada2 denoise-paired\",\n",
    "        \"--i-demultiplexed-seqs \"+directory+\"/\"+output+\"/\"+name+\".demux.qza\",\n",
    "        \"--o-table \"+directory+\"/\"+output+\"/\"+name+\".table.qza\",\n",
    "        \"--o-representative-sequences \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qza\",\n",
    "        \"--o-denoising-stats \"+directory+\"/\"+output+\"/\"+name+\".denoising-stats.qza\",\n",
    "        \"--p-trim-left-f \"+str(trim_dict[name][0]),\n",
    "        \"--p-trim-left-r \"+str(trim_dict[name][2]),\n",
    "        \"--p-trunc-len-f \"+str(trim_dict[name][1]),\n",
    "        \"--p-trunc-len-r \"+str(trim_dict[name][3]),\n",
    "        \"--p-n-threads\",\n",
    "        str(processors)\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DADA2'ed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Create summary of ASVs\n",
    "\n",
    "Drop outputs from below command into https://view.qiime2.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    metadata = dataset[8]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime feature-table summarize\",\n",
    "        \"--i-table \"+directory+\"/\"+output+\"/\"+name+\".table.qza\",\n",
    "        \"--o-visualization \"+directory+\"/\"+output+\"/\"+name+\".table.qzv\",\n",
    "        \"--m-sample-metadata-file \"+directory+\"/\"+metadata\n",
    "    ]))\n",
    "\n",
    "    os.system(' '.join([\n",
    "        \"qiime feature-table tabulate-seqs\",\n",
    "        \"--i-data \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qza\",\n",
    "        \"--o-visualization \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qzv\"\n",
    "    ])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Classify sequences\n",
    "\n",
    "Different QIIME2 versions can conflict with previously donwloaded databases. This section might have to be updated.\n",
    "\n",
    "Download the latest classifiers here: https://docs.qiime2.org/2021.4/data-resources/\n",
    "\n",
    "View output in https://view.qiime2.org\n",
    "\n",
    "Using SILVA v138 pre-built classifier trained on scikit learn 0.24.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example of creating Silva Classifier DB (very slow: use a pre-built one if possible)\n",
    "\n",
    "#### For Silva, remove all the alignment information (unsure of the impact of keeping it) using the following python code:\n",
    "\n",
    "# ```python \n",
    "# import re\n",
    "# from Bio import SeqIO\n",
    "\n",
    "# output = open(\"silva.nr_v128.fasta\", \"w\")\n",
    "\n",
    "# for record in SeqIO.parse(open(\"silva.nr_v128.align\", \"rU\"), \"fasta\") :\n",
    "#     seq = str(record.seq)\n",
    "#     seq = re.sub(\"\\.|-\",\"\",seq)  # Remove \".\" and \"-\"\n",
    "\n",
    "#     output.write(\">\"+record.id+\"\\n\"+seq+\"\\n\")```\n",
    "\n",
    "# #### Import fasta sequence file and taxonomy file as .qza\n",
    "# ```bash\n",
    "# qiime tools import\n",
    "#   --type 'FeatureData[Sequence]'\n",
    "#   --input-path silva.nr_v128.fasta\n",
    "#   --output-path silva.nr_v128.qza\n",
    "\n",
    "# qiime tools import \n",
    "#   --type 'FeatureData[Taxonomy]' \n",
    "#   --source-format HeaderlessTSVTaxonomyFormat \n",
    "#   --input-path silva.nr_v128.tax \n",
    "#   --output-path silva.nr_v128.taxonomy.qza```\n",
    "\n",
    "# #### Run QIIME2 'fit-classifier-naive-bayes'\n",
    "# ```bash\n",
    "# qiime feature-classifier fit-classifier-naive-bayes \n",
    "#   --i-reference-reads silva.nr_v128.qza \n",
    "#   --i-reference-taxonomy silva.nr_v128.taxonomy.qza \n",
    "#   --o-classifier silva.nr_v128.nb.classifier.qza```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use classifier for chosen database for bacteria\n",
    "try:\n",
    "    if db == \"GreenGenes\":\n",
    "        classifier_db = \"/home/db/GreenGenes/qiime2_13.8.99_515.806_nb.classifier.qza\" # out of date\n",
    "    else:\n",
    "        classifier_db = \"~/databases/silva/silva-138-99-515-806-nb-classifier.qza\"\n",
    "except:\n",
    "        classifier_db = \"~/databases/silva/silva-138-99-515-806-nb-classifier.qza\" # default to Silva\n",
    "        \n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    metadata = dataset[8]\n",
    "    domain = dataset[9]\n",
    "\n",
    "    # Classify\n",
    "    if domain == 'bacteria':\n",
    "        os.system(' '.join([\n",
    "            \"qiime feature-classifier classify-sklearn\",\n",
    "            \"--i-classifier\",\n",
    "            classifier_db,\n",
    "            \"--i-reads \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qza\",\n",
    "            \"--o-classification \"+directory+\"/\"+output+\"/\"+name+\".taxonomy.qza\",\n",
    "            \"--p-n-jobs\",\n",
    "            str(processors)\n",
    "        ]))\n",
    "\n",
    "    if domain == 'fungi':\n",
    "        os.system(' '.join([\n",
    "            \"qiime feature-classifier classify-sklearn\",\n",
    "            \"--i-classifier /home/db/UNITE/qiime2_unite_ver7.99_20.11.2016_classifier.qza\", # out of date\n",
    "            \"--i-reads \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qza\",\n",
    "            \"--o-classification \"+directory+\"/\"+output+\"/\"+name+\".taxonomy.qza\",\n",
    "            \"--p-n-jobs\",\n",
    "            str(processors)\n",
    "        ]))\n",
    "\n",
    "    # Output summary\n",
    "    os.system(' '.join([\n",
    "        \"qiime metadata tabulate\",\n",
    "        \"--m-input-file \"+directory+\"/\"+output+\"/\"+name+\".taxonomy.qza\",\n",
    "        \"--o-visualization \"+directory+\"/\"+output+\"/\"+name+\".taxonomy-summary.qzv\"\n",
    "    ])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Merge representative sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repseqslist = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    domain = dataset[6]\n",
    "    \n",
    "    if domain != \"fungi\":\n",
    "        repseqslist.append(\"--i-data \" + os.path.join(directory, output, name+\".rep-seqs.qza\"))\n",
    "\n",
    "os.system(' '.join([\n",
    "    \"qiime feature-table merge-seqs\",\n",
    "    \" \".join(repseqslist),\n",
    "    \"--o-merged-data \"+treepath+\"/\"+treename+\".rep-seqs-merged.qza\"\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11: Make phylogenetic tree (16S only)\n",
    "\n",
    "If dataset is split into multiple libraries, one tree should be created using all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    domain = dataset[6]\n",
    "\n",
    "    # Generate alignment with MAFFT\n",
    "    os.system(' '.join([\n",
    "        \"qiime alignment mafft\",\n",
    "        \"--i-sequences \"+treepath+\"/\"+treename+\".rep-seqs-merged.qza\",\n",
    "        \"--o-alignment \"+treepath+\"/\"+treename+\".rep-seqs-merged-aligned.qza\",\n",
    "        \"--p-n-threads\",\n",
    "        str(processors)\n",
    "    ]))\n",
    "    \n",
    "    # Mask hypervariable regions in alignment\n",
    "    os.system(' '.join([\n",
    "        \"qiime alignment mask\",\n",
    "        \"--i-alignment \"+treepath+\"/\"+treename+\".rep-seqs-merged-aligned.qza\",\n",
    "        \"--o-masked-alignment \"+treepath+\"/\"+treename+\".rep-seqs-merged-aligned-masked.qza\",\n",
    "    ]))\n",
    "    \n",
    "    # Generate tree with FastTree\n",
    "    os.system(' '.join([\n",
    "        \"qiime phylogeny fasttree\",\n",
    "        \"--i-alignment \"+treepath+\"/\"+treename+\".rep-seqs-merged-aligned-masked.qza\",\n",
    "        \"--o-tree \"+treepath+\"/\"+treename+\".tree-unrooted.qza\",\n",
    "        \"--p-n-threads\",\n",
    "        str(processors)\n",
    "    ]))\n",
    "    \n",
    "    # Root the tree\n",
    "    os.system(' '.join([\n",
    "        \"qiime phylogeny midpoint-root\",\n",
    "        \"--i-tree \"+treepath+\"/\"+treename+\".tree-unrooted.qza\",\n",
    "        \"--o-rooted-tree \"+treepath+\"/\"+treename+\".tree-rooted.qza\"\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 12: Reformat taxonomy\n",
    "\n",
    "Define function to reformat taxonomy file to be tidier and to reflect confidence of taxonomic assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_taxonomy(tax_dirty, min_support):\n",
    "    output = open(re.sub(\".tsv\",\"-fixed.tsv\",tax_dirty), \"w\")\n",
    "    \n",
    "    full_rank_length = 7\n",
    "    output.write(\"\\t\".join([\"ASV\",\"Domain\",\"Phylum\",\"Class\",\"Order\",\"Family\",\"Genus\",\"Species\"])+\"\\n\")\n",
    "\n",
    "    with open(tax_dirty, \"r\") as f:\n",
    "        next(f)\n",
    "\n",
    "        for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "\n",
    "                read_id = line[0]\n",
    "                tax_string = line[1]\n",
    "                \n",
    "                ## Remove taxonomy prefixes and underscores (only coded for Silva classifications so far)\n",
    "                if db == \"Silva\":\n",
    "                    tax_string = re.sub(\"[a-z]__\", \"\", tax_string)\n",
    "                    tax_string = re.sub(\"_\", \" \", tax_string)\n",
    "\n",
    "                # Split full rank into ranks\n",
    "                full_rank = tax_string.split(\";\")\n",
    "\n",
    "                ## Identify the lowest classified taxonomic rank\n",
    "                # Account for cases when a taxonomic rank contains an empty space (common in GreenGenes output)\n",
    "                last_classified = full_rank[len(full_rank)-1]            \n",
    "\n",
    "                count = 1\n",
    "                while last_classified == \" \":\n",
    "                    last_classified = full_rank[len(full_rank)-count]\n",
    "                    count = count + 1\n",
    "\n",
    "                # Annotate the last classified as 'putative' if it does not meet the minimum support criteria\n",
    "                if float(line[2]) < float(min_support):\n",
    "                        full_rank[full_rank.index(last_classified)] = \"putative \"+last_classified\n",
    "                        last_classified = \"putative \"+last_classified\n",
    "\n",
    "                #print(last_classified)\n",
    "                # Chloroplast, didn't change because confidence > 0.9\n",
    "\n",
    "                # Add in columns containing unclassified taxonomic information\n",
    "                for n in range(full_rank.index(last_classified)+1, full_rank_length, 1):               \n",
    "                    try:\n",
    "                        full_rank[n] = \"unclassified \"+last_classified\n",
    "                    except:\n",
    "                        full_rank.append(\"unclassified \"+last_classified)\n",
    "\n",
    "                # Write taxonomy to file\n",
    "                output.write(read_id+\"\\t\"+'\\t'.join(full_rank)+\"\\n\")\n",
    "                \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 13: Export from QIIME2\n",
    "\n",
    "Note: Exporting representative sequences is unfinished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    metadata = dataset[8]\n",
    "\n",
    "    # Final output names\n",
    "    fasta_file = directory+\"/\"+output+\"/final/\"+name+\".rep-seqs-final.fasta\"\n",
    "    tax_file = directory+\"/\"+output+\"/final/\"+name+\".taxonomy-final.tsv\"\n",
    "    count_table = directory+\"/\"+output+\"/final/\"+name+\".counts-final.biom\"\n",
    "    \n",
    "    # Make final data directories\n",
    "    if not os.path.isdir(os.path.join(directory, output, \"final\")):\n",
    "        !mkdir $directory/$output/final\n",
    "\n",
    "    # Export taxonomic classifications\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        \"--input-path \"+directory+\"/\"+output+\"/\"+name+\".taxonomy.qza\",\n",
    "        \"--output-path \"+directory+\"/\"+output+\"/\"\n",
    "    ]))\n",
    "    \n",
    "    # Reformat classifications to meet phyloseq format   \n",
    "    format_taxonomy(directory+\"/\"+output+\"/taxonomy.tsv\", min_support)\n",
    "\n",
    "    # Export representative sequences\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        \"--input-path \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qza\",\n",
    "        \"--output-path \"+directory+\"/\"+output+\"/\"\n",
    "    ]))\n",
    "    \n",
    "    # Rename exported files and put in final directory\n",
    "    %mv $directory/$output/dna-sequences.fasta $fasta_file\n",
    "    %mv $directory/$output/feature-table.biom $count_table\n",
    "    %mv $directory/$output/taxonomy-fixed.tsv $tax_file\n",
    "    \n",
    "# Export tree\n",
    "tax_tree = treepath+\"/\"+treename+\".tree-final.nwk\"\n",
    "\n",
    "os.system(' '.join([\n",
    "    \"qiime tools export\",\n",
    "    \"--input-path \"+treepath+\"/\"+treename+\".tree-rooted.qza\",\n",
    "    \"--output-path \"+treepath+\"/\"\n",
    "]))\n",
    "\n",
    "# Rename tree file\n",
    "%mv $treepath/\"tree.nwk\" $tax_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
