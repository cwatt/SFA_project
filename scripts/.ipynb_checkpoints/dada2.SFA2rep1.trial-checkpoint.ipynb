{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: SFA2 rep1 trial\n",
    "\n",
    "Cassandra Wattenburger, 07/07/21\n",
    "\n",
    "### Notes:\n",
    "* Modified from script written by Roli Wilhelm\n",
    "* Running script using QIIME2 v2021.4\n",
    "* This was a trial run to see if the spike-in amount was appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "### Pipeline to process raw sequences into phyloseq object with DADA2 ###\n",
    "* Prep for Import to QIIME2  (Combine two index files)\n",
    "* Import to QIIME2\n",
    "* Demultiplex\n",
    "* Denoise and Merge\n",
    "* Prepare OTU Tables and Rep Sequences  *(Note: sample names starting with a digit will break this step)*\n",
    "* Classify Seqs\n",
    "\n",
    "*100% Appropriated from the \"Atacama Desert Tutorial\" for QIIME2*\n",
    "\n",
    "### Pipeline can handle both 16S rRNA gene and ITS sequences####\n",
    "* Tested on 515f and 806r\n",
    "* Tested on ITS1\n",
    "\n",
    "### Commands to install dependencies ####\n",
    "##### || QIIME2 ||\n",
    "** Note: QIIME2 is still actively in development, and I've noticed frequent new releases. Check for the most up-to-date conda install file <https://docs.qiime2.org/2017.11/install/native/#install-qiime-2-within-a-conda-environment>\n",
    "\n",
    "* wget https://data.qiime2.org/distro/core/qiime2-2020.2-py35-linux-conda.yml\n",
    "* conda env create -n qiime2-2020.2 --file qiime2-2018.2-py35-linux-conda.yml\n",
    "* source activate qiime2-pipeline\n",
    "\n",
    "##### || Copyrighter rrn database ||\n",
    "* The script will automatically install the curated GreenGenes rrn attribute database\n",
    "* https://github.com/fangly/AmpliCopyrighter\n",
    "\n",
    "##### || rpy2 (don't use conda version) ||\n",
    "* pip install rpy2  \n",
    "\n",
    "##### || phyloseq ||\n",
    "* conda install -c r r-igraph \n",
    "* Rscript -e \"source('http://bioconductor.org/biocLite.R');biocLite('phyloseq')\" \n",
    "\n",
    "##### || R packages ||\n",
    "* ape   (natively installed in conda environment)\n",
    "\n",
    "### Citations ###\n",
    "* Caporaso, J. G., Kuczynski, J., Stombaugh, J., Bittinger, K., Bushman, F. D., Costello, E. K., *et al.* (2010). QIIME allows analysis of high-throughput community sequencing data. Nature methods, 7(5), 335-336.\n",
    "\n",
    "* McMurdie and Holmes (2013) phyloseq: An R Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data. PLoS ONE. 8(4):e61217\n",
    "\n",
    "* Paradis E., Claude J. & Strimmer K. 2004. APE: analyses of phylogenetics and evolution in R language. Bioinformatics 20: 289-290.\n",
    "\n",
    "* Angly, F. E., Dennis, P. G., Skarshewski, A., Vanwonterghem, I., Hugenholtz, P., & Tyson, G. W. (2014). CopyRighter: a rapid tool for improving the accuracy of microbial community profiles through lineage-specific gene copy number correction. Microbiome, 2(1), 11.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Prep the sequence data\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Done on the command line.\n",
    "\n",
    "1. Copy/paste the raw sequence data to a new location on the server (never modify original data!)\n",
    "1. Decompress files to .fastq\n",
    "1. Run 'truncate_seqid.sh' on the read1, read2, index1, and index2 fastq files\n",
    "1. Recompress the read1 and read2 files to .fastq.gz (keep the index files uncompressed for a later step)\n",
    "\n",
    "This script removes a portion of the sequence ID that is incompatible with QIIME2. I think it is due to intricacies involved with the BRC's sequencing methods.\n",
    "\n",
    "Will work on incorporating directly into pipeline in future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: User Input\n",
    "\n",
    "### Metadata requirements\n",
    "* Must be located in the project directory\n",
    "* Must be .tsv format \n",
    "* First column named \"SampleID\" for samples\n",
    "* One column named \"BarcodeSequence\" with the relevant barcode seqeunces (rev. comp reverse concatenated with forward barcode sequence)\n",
    "\n",
    "The output directory will be created inside the project directory when you run the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, numpy as np\n",
    "\n",
    "# Prepare an object with the name of the library and all related file paths\n",
    "# datasets = [['name', 'project directory path', 'output directory name', 'modified raw data directory', read1 file name, read2 file name, 'metadata file name','domain of life'], ...]\n",
    "datasets = [['SFA2rep1trial', '/home/cassi/SFAgrowthrate/data_amplicon/SFA2rep1_trial', 'output', \n",
    "             '/home/backup_files/raw_reads/SFA2.cassi.2021/rep1.trial/modified', \n",
    "             'read1_mod.fastq.gz', 'read2_mod.fastq.gz', 'index1_mod.fastq.gz', 'index2_mod.fastq.gz', \n",
    "             'SFA2rep1_metadata.tsv', 'bacteria']]\n",
    "\n",
    "# Set # of processors (10 ussually good)\n",
    "processors = 10\n",
    "\n",
    "# Which bacterial database will you use? Silva or GreenGenes\n",
    "db = \"Silva\"\n",
    "\n",
    "# Phylogenetic tree (non-fungal data)\n",
    "treepath = \"/home/cassi/SFAgrowthrate/data_amplicon/SFA2rep1_trial/tree\" # full path to the phylogenetic tree file to be made\n",
    "treename = \"SFA2rep1trial\" # name prefix for the tree file\n",
    "\n",
    "## Enter minimum support for keeping QIIME classification\n",
    "# Note: Classifications that do not meet this criteria will simply be retained, but labeled 'putative'\n",
    "# Note: I typically use 0.9\n",
    "min_support = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Concatenate barcodes\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Done on command line.\n",
    "\n",
    "1. Run 'concatenate_barcodes_mod.py' on the modified index1 and index2 files\n",
    "1. Recompress all files to .fastq.gz (inlcuding the barcodes.fastq file that you just created)\n",
    "\n",
    "Will work on incorporating this directly into pipeline in future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Move data to output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    raw = dataset[3]\n",
    "    read1 = dataset[4]\n",
    "    read2 = dataset[5]\n",
    "    \n",
    "    # Create output directory if it doesn't exist already\n",
    "    if not os.path.isdir(os.path.join(directory, output)):\n",
    "        !mkdir $directory/$output\n",
    "    \n",
    "    # Create a symbolic link to the read data (files too big, copy/paste a waste of space)\n",
    "    # QIIME2 import requires a directory containing files named: forward.fastq.gz, reverse.fastq.gz and barcodes.fastq.gz \n",
    "    !ln -s $raw/$read1 $directory/$output/forward.fastq.gz\n",
    "    !ln -s $raw/$read2 $directory/$output/reverse.fastq.gz\n",
    "    \n",
    "    # Move concatenated barcodes to project directory\n",
    "    !cp $raw/barcodes.fastq.gz $directory/$output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Import into QIIME2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime tools import\",\n",
    "        \"--type EMPPairedEndSequences\",\n",
    "        \"--input-path \"+directory+\"/\"+output,\n",
    "        \"--output-path \"+directory+\"/\"+output+\"/\"+name+\".qza\"\n",
    "    ]))\n",
    "    \n",
    "    # This more direct command is broken by the fact QIIME uses multiple dashes in their arguments (is my theory)\n",
    "    #!qiime tools import --type EMPPairedEndSequences --input-path $directory/output --output-path $directory/output/$name.qza\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Demultiplex\n",
    "\n",
    "### Notes\n",
    "* The barcode you supply to QIIME is now a concatenation of your forward and reverse barcode\n",
    "* Your 'forward' barcode is actually the reverse complement of your reverse barcode and the 'reverse' is your forward barcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SLOW STEP\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    metadata = dataset[8]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime demux emp-paired\",\n",
    "        \"--m-barcodes-file \"+directory+\"/\"+metadata,\n",
    "        \"--m-barcodes-column BarcodeSequence\",\n",
    "        \"--p-no-golay-error-correction\",\n",
    "        \"--i-seqs \"+directory+\"/\"+output+\"/\"+name+\".qza\",\n",
    "        \"--o-per-sample-sequences \"+directory+\"/\"+output+\"/\"+name+\".demux\",\n",
    "        \"--o-error-correction-details \"+directory+\"/\"+output+\"/\"+name+\".demux-details.qza\"\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Demultiplexing is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Visualize quality scores\n",
    "\n",
    "Drop output from below command into https://view.qiime2.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime demux summarize\",\n",
    "        \"--i-data \"+directory+\"/\"+output+\"/\"+name+\".demux.qza\",\n",
    "        \"--o-visualization \"+directory+\"/\"+output+\"/\"+name+\".demux.qzv\"\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Trimming parameters | USER INPUT REQUIRED\n",
    "\n",
    "Based on the quality scores of the bp along the reads, choose trim and truncate values. Trim refers to the start of a sequence and truncate the total length (i.e. number of bases to remove from end).\n",
    "\n",
    "All trimming parameters must be the same for datasets that will be directly compared to one-another because ASVs are determined by sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## User Input Required\n",
    "\n",
    "## Input your trimming parameters into a python dictionary for all libraries\n",
    "# trim_dict = {\"LibraryName1\":[trim_forward, truncate_forward, trim_reverse, truncate_reverse],\n",
    "#              \"LibraryName2\":[trim_forward, truncate_forward, trim_reverse, truncate_reverse],\n",
    "#               etc...}\n",
    "\n",
    "# The example in the Atacama Desert Tutorial trims 13 bp from the start of each read and does not remove any bases from the end of the 150 bp reads:\n",
    "#  --p-trim-left-f 13 \\  \n",
    "#  --p-trim-left-r 13 \\\n",
    "#  --p-trunc-len-f 150 \\\n",
    "#  --p-trunc-len-r 150\n",
    "\n",
    "trim_dict = {\"SFA2rep1trial\":[10,240,10,220]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Trim, denoise and join (aka 'merge') reads using DADA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SLOW STEP\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime dada2 denoise-paired\",\n",
    "        \"--i-demultiplexed-seqs \"+directory+\"/\"+output+\"/\"+name+\".demux.qza\",\n",
    "        \"--o-table \"+directory+\"/\"+output+\"/\"+name+\".table.qza\",\n",
    "        \"--o-representative-sequences \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qza\",\n",
    "        \"--o-denoising-stats \"+directory+\"/\"+output+\"/\"+name+\".denoising-stats.qza\",\n",
    "        \"--p-trim-left-f \"+str(trim_dict[name][0]),\n",
    "        \"--p-trim-left-r \"+str(trim_dict[name][2]),\n",
    "        \"--p-trunc-len-f \"+str(trim_dict[name][1]),\n",
    "        \"--p-trunc-len-r \"+str(trim_dict[name][3]),\n",
    "        \"--p-n-threads\",\n",
    "        str(processors)\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DADA2'ed\n"
     ]
    }
   ],
   "source": [
    "print(\"DADA2'ed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Create summary of ASVs\n",
    "\n",
    "Drop outputs from below command into https://view.qiime2.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    metadata = dataset[8]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime feature-table summarize\",\n",
    "        \"--i-table \"+directory+\"/\"+output+\"/\"+name+\".table.qza\",\n",
    "        \"--o-visualization \"+directory+\"/\"+output+\"/\"+name+\".table.qzv\",\n",
    "        \"--m-sample-metadata-file \"+directory+\"/\"+metadata\n",
    "    ]))\n",
    "\n",
    "    os.system(' '.join([\n",
    "        \"qiime feature-table tabulate-seqs\",\n",
    "        \"--i-data \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qza\",\n",
    "        \"--o-visualization \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qzv\"\n",
    "    ])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Classify sequences\n",
    "\n",
    "Different QIIME2 versions can conflict with previously donwloaded databases. This section might have to be updated.\n",
    "\n",
    "Download the latest classifiers here: https://docs.qiime2.org/2021.4/data-resources/\n",
    "\n",
    "View output in https://view.qiime2.org\n",
    "\n",
    "Using SILVA v138 pre-built classifier trained on scikit learn 0.24.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example of creating Silva Classifier DB (very slow: use a pre-built one if possible)\n",
    "\n",
    "#### For Silva, remove all the alignment information (unsure of the impact of keeping it) using the following python code:\n",
    "\n",
    "# ```python \n",
    "# import re\n",
    "# from Bio import SeqIO\n",
    "\n",
    "# output = open(\"silva.nr_v128.fasta\", \"w\")\n",
    "\n",
    "# for record in SeqIO.parse(open(\"silva.nr_v128.align\", \"rU\"), \"fasta\") :\n",
    "#     seq = str(record.seq)\n",
    "#     seq = re.sub(\"\\.|-\",\"\",seq)  # Remove \".\" and \"-\"\n",
    "\n",
    "#     output.write(\">\"+record.id+\"\\n\"+seq+\"\\n\")```\n",
    "\n",
    "# #### Import fasta sequence file and taxonomy file as .qza\n",
    "# ```bash\n",
    "# qiime tools import\n",
    "#   --type 'FeatureData[Sequence]'\n",
    "#   --input-path silva.nr_v128.fasta\n",
    "#   --output-path silva.nr_v128.qza\n",
    "\n",
    "# qiime tools import \n",
    "#   --type 'FeatureData[Taxonomy]' \n",
    "#   --source-format HeaderlessTSVTaxonomyFormat \n",
    "#   --input-path silva.nr_v128.tax \n",
    "#   --output-path silva.nr_v128.taxonomy.qza```\n",
    "\n",
    "# #### Run QIIME2 'fit-classifier-naive-bayes'\n",
    "# ```bash\n",
    "# qiime feature-classifier fit-classifier-naive-bayes \n",
    "#   --i-reference-reads silva.nr_v128.qza \n",
    "#   --i-reference-taxonomy silva.nr_v128.taxonomy.qza \n",
    "#   --o-classifier silva.nr_v128.nb.classifier.qza```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use classifier for chosen database for bacteria\n",
    "try:\n",
    "    if db == \"GreenGenes\":\n",
    "        classifier_db = \"/home/db/GreenGenes/qiime2_13.8.99_515.806_nb.classifier.qza\" # out of date\n",
    "    else:\n",
    "        classifier_db = \"~/databases/silva/silva-138-99-515-806-nb-classifier.qza\"\n",
    "except:\n",
    "        classifier_db = \"~/databases/silva/silva-138-99-515-806-nb-classifier.qza\" # default to Silva\n",
    "        \n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    metadata = dataset[8]\n",
    "    domain = dataset[9]\n",
    "\n",
    "    # Classify\n",
    "    if domain == 'bacteria':\n",
    "        os.system(' '.join([\n",
    "            \"qiime feature-classifier classify-sklearn\",\n",
    "            \"--i-classifier\",\n",
    "            classifier_db,\n",
    "            \"--i-reads \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qza\",\n",
    "            \"--o-classification \"+directory+\"/\"+output+\"/\"+name+\".taxonomy.qza\",\n",
    "            \"--p-n-jobs\",\n",
    "            str(processors)\n",
    "        ]))\n",
    "\n",
    "    if domain == 'fungi':\n",
    "        os.system(' '.join([\n",
    "            \"qiime feature-classifier classify-sklearn\",\n",
    "            \"--i-classifier /home/db/UNITE/qiime2_unite_ver7.99_20.11.2016_classifier.qza\", # out of date\n",
    "            \"--i-reads \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qza\",\n",
    "            \"--o-classification \"+directory+\"/\"+output+\"/\"+name+\".taxonomy.qza\",\n",
    "            \"--p-n-jobs\",\n",
    "            str(processors)\n",
    "        ]))\n",
    "\n",
    "    # Output summary\n",
    "    os.system(' '.join([\n",
    "        \"qiime metadata tabulate\",\n",
    "        \"--m-input-file \"+directory+\"/\"+output+\"/\"+name+\".taxonomy.qza\",\n",
    "        \"--o-visualization \"+directory+\"/\"+output+\"/\"+name+\".taxonomy-summary.qzv\"\n",
    "    ])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Merge representative sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qiime feature-table merge-seqs --i-data ~/SFAgrowthrate/data_amplicon/SFA2rep1_trial/output/SFA2rep1trial.rep-seqs.qza --o-merged-data /home/cassi/SFAgrowthrate/data_amplicon/SFA2rep1_trial/tree/SFA2rep1trial.rep-seqs-merged.qza\n"
     ]
    }
   ],
   "source": [
    "repseqslist = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    domain = dataset[6]\n",
    "    \n",
    "    if domain != \"fungi\":\n",
    "        repseqslist.append(\"--i-data \" + os.path.join(directory, output, name+\".rep-seqs.qza\"))\n",
    "\n",
    "os.system(' '.join([\n",
    "    \"qiime feature-table merge-seqs\",\n",
    "    \" \".join(repseqslist),\n",
    "    \"--o-merged-data \"+treepath+\"/\"+treename+\".rep-seqs-merged.qza\"\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11: Make phylogenetic tree (16S only)\n",
    "\n",
    "If dataset is split into multiple libraries, one tree should be created using all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    domain = dataset[6]\n",
    "\n",
    "    # Generate alignment with MAFFT\n",
    "    os.system(' '.join([\n",
    "        \"qiime alignment mafft\",\n",
    "        \"--i-sequences \"+treepath+\"/\"+treename+\".rep-seqs-merged.qza\",\n",
    "        \"--o-alignment \"+treepath+\"/\"+treename+\".rep-seqs-merged-aligned.qza\",\n",
    "        \"--p-n-threads\",\n",
    "        str(processors)\n",
    "    ]))\n",
    "    \n",
    "    # Mask hypervariable regions in alignment\n",
    "    os.system(' '.join([\n",
    "        \"qiime alignment mask\",\n",
    "        \"--i-alignment \"+treepath+\"/\"+treename+\".rep-seqs-merged-aligned.qza\",\n",
    "        \"--o-masked-alignment \"+treepath+\"/\"+treename+\".rep-seqs-merged-aligned-masked.qza\",\n",
    "    ]))\n",
    "    \n",
    "    # Generate tree with FastTree\n",
    "    os.system(' '.join([\n",
    "        \"qiime phylogeny fasttree\",\n",
    "        \"--i-alignment \"+treepath+\"/\"+treename+\".rep-seqs-merged-aligned-masked.qza\",\n",
    "        \"--o-tree \"+treepath+\"/\"+treename+\".tree-unrooted.qza\",\n",
    "        \"--p-n-threads\",\n",
    "        str(processors)\n",
    "    ]))\n",
    "    \n",
    "    # Root the tree\n",
    "    os.system(' '.join([\n",
    "        \"qiime phylogeny midpoint-root\",\n",
    "        \"--i-tree \"+treepath+\"/\"+treename+\".tree-unrooted.qza\",\n",
    "        \"--o-rooted-tree \"+treepath+\"/\"+treename+\".tree-rooted.qza\"\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 12: Reformat taxonomy\n",
    "\n",
    "Define function to reformat taxonomy file to be tidier and to reflect confidence of taxonomic assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_taxonomy(tax_dirty, min_support):\n",
    "    output = open(re.sub(\".tsv\",\"-fixed.tsv\",tax_dirty), \"w\")\n",
    "    \n",
    "    full_rank_length = 7\n",
    "    output.write(\"\\t\".join([\"ASV\",\"Domain\",\"Phylum\",\"Class\",\"Order\",\"Family\",\"Genus\",\"Species\"])+\"\\n\")\n",
    "\n",
    "    with open(tax_dirty, \"r\") as f:\n",
    "        next(f)\n",
    "\n",
    "        for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "\n",
    "                read_id = line[0]\n",
    "                tax_string = line[1]\n",
    "                \n",
    "                ## Remove taxonomy prefixes and underscores (only coded for Silva classifications so far)\n",
    "                if db == \"Silva\":\n",
    "                    tax_string = re.sub(\"[a-z]__\", \"\", tax_string)\n",
    "                    tax_string = re.sub(\"_\", \" \", tax_string)\n",
    "\n",
    "                # Split full rank into ranks\n",
    "                full_rank = tax_string.split(\";\")\n",
    "\n",
    "                ## Identify the lowest classified taxonomic rank\n",
    "                # Account for cases when a taxonomic rank contains an empty space (common in GreenGenes output)\n",
    "                last_classified = full_rank[len(full_rank)-1]            \n",
    "\n",
    "                count = 1\n",
    "                while last_classified == \" \":\n",
    "                    last_classified = full_rank[len(full_rank)-count]\n",
    "                    count = count + 1\n",
    "\n",
    "                # Annotate the last classified as 'putative' if it does not meet the minimum support criteria\n",
    "                if float(line[2]) < float(min_support):\n",
    "                        full_rank[full_rank.index(last_classified)] = \"putative \"+last_classified\n",
    "                        last_classified = \"putative \"+last_classified\n",
    "\n",
    "                #print(last_classified)\n",
    "                # Chloroplast, didn't change because confidence > 0.9\n",
    "\n",
    "                # Add in columns containing unclassified taxonomic information\n",
    "                for n in range(full_rank.index(last_classified)+1, full_rank_length, 1):               \n",
    "                    try:\n",
    "                        full_rank[n] = \"unclassified \"+last_classified\n",
    "                    except:\n",
    "                        full_rank.append(\"unclassified \"+last_classified)\n",
    "\n",
    "                # Write taxonomy to file\n",
    "                output.write(read_id+\"\\t\"+'\\t'.join(full_rank)+\"\\n\")\n",
    "                \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 13: Export from QIIME2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    metadata = dataset[8]\n",
    "\n",
    "    # Final output names\n",
    "    fasta_file = directory+\"/\"+output+\"/final/\"+name+\".rep-seqs-final.fasta\"\n",
    "    tax_file = directory+\"/\"+output+\"/final/\"+name+\".taxonomy-final.tsv\"\n",
    "    count_table = directory+\"/\"+output+\"/final/\"+name+\".counts-final.biom\"\n",
    "    \n",
    "    # Make final data directories\n",
    "    if not os.path.isdir(os.path.join(directory, output, \"final\")):\n",
    "        !mkdir $directory/$output/final\n",
    "    \n",
    "    # ???\n",
    "    #with open(os.path.join(directory, output, name+\"repseqslist\"), \"w\") as outfile:\n",
    "    #    for i in repseqslist:\n",
    "    #        outfile.write(i+\"\\n\")\n",
    "\n",
    "    # Export taxonomic classifications\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        \"--input-path \"+directory+\"/\"+output+\"/\"+name+\".taxonomy.qza\",\n",
    "        \"--output-path \"+directory+\"/\"+output+\"/\"\n",
    "    ]))\n",
    "    \n",
    "    # Reformat classifications to meet phyloseq format   \n",
    "    format_taxonomy(directory+\"/\"+output+\"/taxonomy.tsv\", min_support)\n",
    "\n",
    "    # Export ASV table\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        \"--input-path \"+directory+\"/\"+output+\"/\"+name+\".table.qza\",\n",
    "        \"--output-path \"+directory+\"/\"+output+\"/\"\n",
    "    ]))\n",
    "\n",
    "    # Export ASV sequences\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        \"--input-path \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qza\",\n",
    "        \"--output-path \"+directory+\"/\"+output+\"/\"\n",
    "    ]))\n",
    "    \n",
    "    # Rename exported files and put in final directory\n",
    "    %mv $directory/$output/dna-sequences.fasta $fasta_file\n",
    "    %mv $directory/$output/feature-table.biom $count_table\n",
    "    %mv $directory/$output/taxonomy-fixed.tsv $tax_file\n",
    "    \n",
    "# Export tree\n",
    "tax_tree = treepath+\"/\"+treename+\".tree-final.nwk\"\n",
    "\n",
    "os.system(' '.join([\n",
    "    \"qiime tools export\",\n",
    "    \"--input-path \"+treepath+\"/\"+treename+\".tree-rooted.qza\",\n",
    "    \"--output-path \"+treepath+\"/\"\n",
    "]))\n",
    "\n",
    "# Rename tree file\n",
    "%mv $treepath/\"tree.nwk\" $tax_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
