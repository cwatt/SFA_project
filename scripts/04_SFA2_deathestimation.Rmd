---
title: "SFA2 - Death estimations"
author: "Cassandra Wattenburger"
date: "11/15/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = "show")
knitr::opts_chunk$set(message = FALSE)
```

```{r}
# Clear working directory, load in packages, generate package info
rm(list=ls())

library("tidyverse")
library("trelliscopejs")

sessionInfo()
```

# Import data

* The already prepped data from 03_SFA2_growthestimation.Rmd
* Taxonomy
* Metadata

```{r}
# Relational abundances, prepped
norm_prepped <- readRDS("../data_intermediate/SFA2_norm_prepped.rds")

# Taxonomy
tax <- readRDS("../data_intermediate/SFA2_taxonomy.rds")

# Metadata
meta <- readRDS("../data_intermediate/SFA2_metadata.rds")
```

# Estimate death - from start of incubation

I noticed many time series where taxa seemed to die off immediately during the incubation.

Create saving function:

```{r}
# Function: Saves chosen estimate from below algorhithm
save_fit <- function(label, start, end, df_sub, output) {
  # Save estimate info
  est <- NULL; coeff <- NULL; residuals <- NULL; pval <- NULL; thisrow <- data.frame() # clear previous
  est <- lm(abund ~ time, data=df_sub[start:end,])
  coeff <- as.numeric(est$coefficients[2])
  residuals <- sum(abs(resid(est)))
  pval <- summary(est)$coefficients[2,4]
  thisrow <- data.frame(label, start, end, coeff, pval, residuals)
  output <- bind_rows(output, thisrow)
  return(output)
}
```

Estimate death:

* Same as algorhithm from 03_SFA2_deathestimation.Rmd but looking for negative slopes instead (indicate death)
* Only estimating on data for taxa with estimated growth, 

```{r}
## Requires a dataframe with columns containing a unique label for each data point, abundance values, time points
## df = data frame containing time series with abundance values (ln transformed), long format
## df must contain columns named:
### label = column with unique identifier for each time series
### abund = column with abundance values at each time point
### time = column with time point values
df <- norm_prepped
death_estimates <- data.frame()
for (label in as.character(unique(df$label))) {
    
  # Subset one time series using the label
  df_sub <- data.frame()
  df_sub <- df[df$label==label,] 
  stop <- FALSE
    
  # Sliding window
  for (start in 1:(nrow(df_sub) - 3)) {
    stop <- FALSE
    for (end in (start + 3):nrow(df_sub)) {
      if (stop == TRUE) {break}
     
      # Fit linear model to the window
      window_lm <- NULL; window_p <- NULL; window_coeff <- NULL
      window_lm <- lm(abund ~ time, data = df_sub[start:end,])
      window_p <- summary(window_lm)$coefficients[2,4]
      window_coeff <- window_lm$coefficients[2]
  
      # If a suitable fit is found and more time points exist that were not included, try extending the window
      if (window_p <= 0.05 & window_coeff < 0 & end < nrow(df_sub)) {
        for (extend_end in ((end+1):nrow(df_sub))) {
          if (stop == TRUE) {break}
            
          # Fit linear model to previous, non-extended window
          prevwindow_lm <- NULL; prevwindow_p <- NULL; prevwindow_coeff <- NULL 
          prevwindow_lm <- lm(abund ~ time, data = df_sub[start:(extend_end - 1),])
          prevwindow_p <- summary(prevwindow_lm)$coefficients[2,4]
          prevwindow_coeff <- prevwindow_lm$coefficients[2]
            
          # Fit linear model to the extended window
          newwindow_lm <- NULL; newwindow_p <- NULL; newwindow_coeff <- NULL 
          newwindow_lm <- lm(abund ~ time, data = df_sub[start:extend_end,])
          newwindow_p <- summary(newwindow_lm)$coefficients[2,4]
          newwindow_coeff <- newwindow_lm$coefficients[2]
            
          # If see improvement and can add more data, continue extending the window
          if (newwindow_p <= prevwindow_p & newwindow_coeff < 0 & extend_end < nrow(df_sub)) {
            next
          }
            
          # If no improvement, save the previous fit
          else if (newwindow_p > prevwindow_p & prevwindow_coeff < 0 & extend_end < nrow(df_sub)) {
            end <- extend_end - 1
            death_estimates <- save_fit(label, start, end, df_sub, death_estimates)
            stop <- TRUE
          }
            
          # If see improvement (or no harm) but no more data points to fit, save the extended fit
          else if (newwindow_p <= prevwindow_p & newwindow_coeff < 0 & extend_end == nrow(df_sub)) {
            end <- extend_end
            death_estimates <- save_fit(label, start, end, df_sub, death_estimates)
            stop <- TRUE
          } 
        }
      }
        
      # If no more data available to add to model, save the fit
      else if (window_p <= 0.05 & window_coeff < 0 & end == nrow(df_sub)) {
        death_estimates <- save_fit(label, start, end, df_sub, death_estimates)
        stop <- TRUE
      }
    }
  }
}

dim(death_estimates)
```

### Remove "essentially perfect fits"

I can't find any guidance on how to determine whether or not a fit is "perfect" but I know that the residuals are essentially equal to 0 for perfect fits. I'll use 0.0001 as a filtering threshold for removal.

```{r}
# Remove "perfect" fits as precaution
death_estimates <- filter(death_estimates, residuals >= 0.0001)

dim(death_estimates)
```

### Select best fit for each time series

* Lowest p-value of the slope

```{r}
# Choose lowest p-value window
lowest_pvals <- death_estimates %>% 
  group_by(label) %>% 
  summarize(pval = min(pval)) %>% 
  ungroup()

dim(lowest_pvals)

# Filter chosen estimates
death_estimates <- death_estimates %>% 
  semi_join(lowest_pvals)

dim(death_estimates)
```

# False positive control

Histogram of quality filtered p-values from actual estimates:

```{r}
hist(death_estimates$pval, xlab="P-values", main="Histogram of quality filtered p-values")
```

This doesn't look great.

See: http://varianceexplained.org/statistics/interpreting-pvalue-histogram/

I'm choosing to use a permutation approach where I use my estimating algorhithmhmhm on randomly generated data with characteristics of the real data. I'll use this false positive information to filter my real estimates. Traditional false positive control methods are far too conservative for my dataset.

### Simulate random data

Completely random data designed to reflect actual data, if we detect "significant" death rate estimates from this, we must control for that, because the same thing can happen in our actual data.

Import previously generated simulated time series, see 03_SFA2_deathestimation.Rmd

```{r}
sim_data <- readRDS("../data_intermediate/SFA2_simulated.rds")
```

### Estimate "death" on simulated data

```{r}
# Prep simulated data (rename columns)
sim_prepped <- sim_data %>% 
  select(label=simulation, abund=rand_abund, time=rand_day)

# Run algorithinmmmnmnm
df <- sim_prepped
sim_estimates <- data.frame()
for (label in as.character(unique(df$label))) {
    
  # Subset one time series using the label
  df_sub <- data.frame()
  df_sub <- df[df$label==label,] 
  stop <- FALSE
    
  # Sliding window
  for (start in 1:(nrow(df_sub) - 3)) {
    stop <- FALSE
    for (end in (start + 3):nrow(df_sub)) {
      if (stop == TRUE) {break}
     
      # Fit linear model to the window
      window_lm <- NULL; window_p <- NULL; window_coeff <- NULL
      window_lm <- lm(abund ~ time, data = df_sub[start:end,])
      window_p <- summary(window_lm)$coefficients[2,4]
      window_coeff <- window_lm$coefficients[2]
  
      # If a suitable fit is found and more time points exist that were not included, try extending the window
      if (window_p <= 0.05 & window_coeff < 0 & end < nrow(df_sub)) {
        for (extend_end in ((end+1):nrow(df_sub))) {
          if (stop == TRUE) {break}
            
          # Fit linear model to previous, non-extended window
          prevwindow_lm <- NULL; prevwindow_p <- NULL; prevwindow_coeff <- NULL 
          prevwindow_lm <- lm(abund ~ time, data = df_sub[start:(extend_end - 1),])
          prevwindow_p <- summary(prevwindow_lm)$coefficients[2,4]
          prevwindow_coeff <- prevwindow_lm$coefficients[2]
            
          # Fit linear model to the extended window
          newwindow_lm <- NULL; newwindow_p <- NULL; newwindow_coeff <- NULL 
          newwindow_lm <- lm(abund ~ time, data = df_sub[start:extend_end,])
          newwindow_p <- summary(newwindow_lm)$coefficients[2,4]
          newwindow_coeff <- newwindow_lm$coefficients[2]
            
          # If see improvement and can add more data, continue extending the window
          if (newwindow_p <= prevwindow_p & newwindow_coeff < 0 & extend_end < nrow(df_sub)) {
            next
          }
            
          # If no improvement, save the previous fit
          else if (newwindow_p > prevwindow_p & prevwindow_coeff < 0 & extend_end < nrow(df_sub)) {
            end <- extend_end - 1
            sim_estimates <- save_fit(label, start, end, df_sub, sim_estimates)
            stop <- TRUE
          }
            
          # If see improvement (or no harm) but no more data points to fit, save the extended fit
          else if (newwindow_p <= prevwindow_p & newwindow_coeff < 0 & extend_end == nrow(df_sub)) {
            end <- extend_end
            sim_estimates <- save_fit(label, start, end, df_sub, sim_estimates)
            stop <- TRUE
          } 
        }
      }
        
      # If no more data available to add to model, save the fit
      else if (window_p <= 0.05 & window_coeff < 0 & end == nrow(df_sub)) {
        sim_estimates <- save_fit(label, start, end, df_sub, sim_estimates)
        stop <- TRUE
      }
    }
  }
}
dim(sim_estimates)
```

### Remove "essentially perfect fits"

I can't find any guidance on how to determine whether or not a fit is "perfect" but I know that the residuals are essentially equal to 0 for perfect fits. I'll use 0.0001 as a filtering threshold for removal.

```{r}
# Remove "perfect" fits as precaution
sim_estimates <- filter(sim_estimates, residuals >= 0.0001)

dim(sim_estimates)
```

### Select best fit for each time series

* Lowest p-value of the slope

```{r}
# Choose lowest p-value window
sim_lowest_pvals <- sim_estimates %>% 
  group_by(label) %>% 
  summarize(pval = min(pval)) %>% 
  ungroup()

dim(sim_lowest_pvals)

# Filter chosen estimates
sim_estimates <- sim_estimates %>% 
  semi_join(sim_lowest_pvals)

dim(sim_estimates)
```

False positive rates:

```{r}
# False positives
a <- nrow(sim_estimates[sim_estimates$pval <= 0.05,])
b <- nrow(sim_estimates[sim_estimates$pval <= 0.025,])
c <- nrow(sim_estimates[sim_estimates$pval <= 0.01,])
d <- nrow(sim_estimates[sim_estimates$pval <= 0.005,])
e <- nrow(sim_estimates[sim_estimates$pval <= 0.001,])
f <- nrow(sim_estimates[sim_estimates$pval <= 0.0005,])

false_pos <- data.frame(c(0.05, 0.025, 0.01, 0.005, 0.001, 0.0005), c(a,b,c,d,e,f))
colnames(false_pos)=c("pvalue","false")

ggplot(false_pos, aes(x=pvalue, y=false)) +
  geom_point() +
  geom_smooth(method="lm") +
  theme_test() +
  labs(title="Relationship between p-value and number of false positives", x="P-value", y="False positives")
```

Find 10%, 5%, 2.5%, 1% false positive allowance p-value thresholds

* Using linear model to predict

```{r}
falsepos_lm <- lm(false ~ pvalue, data=false_pos)
falsepos_lm
```

```{r}
# Extract model coefficients
slope <- as.numeric(falsepos_lm$coefficients[2])
intercept <- as.numeric(falsepos_lm$coefficients[1])

# 10% false positive allowance
false10_pval <- (100 - intercept)/slope

## 5% false positive allowance
false5_pval <- (50 - intercept)/slope

# 2.5% false positive allowance
false2.5_pval <- (25 - intercept)/slope

# 1% false positive allowance
false1_pval <- (10 - intercept)/slope
```

Filter p-values based on cut-offs determined (10%, 5%, 2.5%, 1%):

```{r}
# ~10% false positives
death_falsepos10 <- subset(death_estimates, pval <= false10_pval)
nrow(death_falsepos10)

# ~5% false positives
death_falsepos5 <- subset(death_estimates, pval <= false5_pval)
nrow(death_falsepos5)

# 2.5%
death_falsepos2.5 <- subset(death_estimates, pval <= false2.5_pval)
nrow(death_falsepos2.5)

# 1%
death_falsepos1 <- subset(death_estimates, pval <= false1_pval)
nrow(death_falsepos1)
```

Going with 10% again to match growth estimations and conserve largest number of estimates.

# Calculate death metrics

### Start and end day, change in abundance

```{r}
# Convert start and end to actual day
death_final <- data.frame()
for (l in as.character(unique(death_falsepos10$label))) {
  # Isolate timeseries
  norm_label <- norm_prepped %>% 
    filter(label==l) %>% 
    arrange(time)
  death_label <- filter(death_falsepos10, label==l)
  start <- death_label$start
  end <- death_label$end
  # Start and end day of death
  start_day <- norm_label[start,]$time
  end_day <- norm_label[end,]$time
  # Starting and ending relational abundance
  start_abund <- norm_label[start,]$norm_abund_avg
  end_abund <- norm_label[end,]$norm_abund_avg
  change_abund <- end_abund - start_abund
  # Save output
  this_row <- bind_cols(label = as.character(death_label$label), k = death_label$coeff,
                        start_pt = death_label$start, end_pt = death_label$end,
                        start_day = start_day, end_day = end_day, 
                        start_abund = start_abund, end_abund = end_abund, change_abund = change_abund)
  death_final <- bind_rows(death_final, this_row)
}
```

# Tidy up and save data

Relational abundances:

```{r}
# Labels of estimated taxa
est_labels <- as.character(death_estimates$label)

# Extract from relational abundance table and clean up
norm_tidy <- norm_prepped %>% 
  filter(label %in% est_labels) %>% 
  select(label, everything(), ln_norm_abund_avg=abund, Day=time) %>% 
  select(label, Inoculant, ASV, Day, norm_abund_avg, ln_norm_abund_avg)
```

```{r, eval=FALSE}
saveRDS(norm_tidy, file="../data_intermediate/SFA2_norm_deestimated.rds")
```

Death estimates:

```{r}
# Add back metadata
death_tidy <- death_final %>% 
  mutate(DOC = if_else(grepl("high", label), "high", "low"),
         Inoculant = gsub("([0-9]+)_.+", "\\1", label),
         ASV = gsub("[0-9]+_(.+)", "\\1", label)) %>% 
  select(label, Inoculant, ASV, k:change_abund)
```

```{r, eval=FALSE}
saveRDS(death_tidy, file="../data_intermediate/SFA2_death_estimates.rds")
```

# Create list of ASVs for PAPRICA

```{r, eval=FALSE}
death_asvs <- select(death_tidy, ASV)
write_tsv(death_asvs, file="../paprica_analysis/death_asvs.txt")
```

# Plot death estimates

```{r}
# For messing with graphs
death_testing <- head(death_tidy)

count <- 0

for (l in as.character(death_tidy$label)) {
  count <- count + 1
  # Subset time series
  death_label <- filter(death_tidy, label==l)
  norm_label <- filter(norm_tidy, label==l) %>% 
  arrange(Day)
  # Title information
  asv <- death_label$ASV
  tax_info <- filter(tax, ASV == asv)
  
  title <- paste0(count, ". ", tax_info$Phylum, ", ", tax_info$Genus)
  # Graph with estimate
  graph <- ggplot(norm_label, aes(x=Day, y=ln_norm_abund_avg)) +
    geom_point(shape=1, size=2, color="#6F7378") +
    geom_line(color="#6F7378") +
    geom_smooth(method="lm", data=norm_label[death_label$start_pt:death_label$end_pt,], linetype=2, color="black") +
    labs(title=title, x="Day", y="ln Relational abundance") +
    theme_test() +
    theme(title = element_text(size=18),
        axis.title = element_text(size=16),
        axis.text = element_text(size=14))
  
  # Print/save
  print(graph)
  #path <- "../figures/grestimates_10fpr3window/"
  #filename <- paste0(count, "_", tax_info$Phylum, "_", tax_info$Genus, ".png")
  #ggsave(plot=graph, filename=paste0(path, filename), device="png")

  }
```

# Plot growth and death

```{r}
# Import growth estimates and relational abundances
growth <- readRDS("../data_intermediate/SFA2_growth_estimates.rds") %>% 
  select(everything(), growth_k=k, growth_start_pt=start_pt, growth_end_pt=end_pt,
         growth_start_day=start_day, growth_end_day=end_day,
         growth_start_abund=start_abund, growth_end_abund=end_abund, growth_change_abund=change_abund)
norm_growth <- readRDS("../data_intermediate/SFA2_norm_grestimated.rds")

# Rename death columns
death_rename <- death_tidy %>% 
  select(everything(), death_k=k, death_start_pt=start_pt, death_end_pt=end_pt,
         death_start_day=start_day, death_end_day=end_day,
         death_start_abund=start_abund, death_end_abund=end_abund, death_change_abund=change_abund)

# Combine growth and death
both <- inner_join(growth, death_rename)
norm_both<- filter(norm_growth, label %in% unique(as.character(norm_tidy$label)))

# Plot

# For messing with graphs
both_testing <- head(both)

for (l in as.character(both$label)) {
  # Subset time series
  both_label <- filter(both, label==l)
  norm_label <- filter(norm_both, label==l) %>% 
    arrange(Day)
  # Title information
  asv <- death_label$ASV
  tax_info <- filter(tax, ASV == asv)
  title <- paste0(tax_info$Phylum, ", ", tax_info$Genus)
  # Graph with estimate
  graph <- ggplot(norm_label, aes(x=Day, y=ln_norm_abund_avg)) +
    geom_point(shape=1, size=2, color="#6F7378") +
    geom_line(color="#6F7378") +
    geom_smooth(method="lm", data=norm_label[both_label$growth_start_pt:both_label$growth_end_pt,], linetype=2, color="#005427") +
    geom_smooth(method="lm", data=norm_label[both_label$death_start_pt:both_label$death_end_pt,], linetype=2, color="#8C0500") +
    labs(title=title, x="Day", y="ln Relational abundance") +
    theme_test() +
    theme(title = element_text(size=18),
        axis.title = element_text(size=16),
        axis.text = element_text(size=14))
  print(graph)
}

```

