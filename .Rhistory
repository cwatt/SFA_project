graph_timeseries(norm_prep, "0924a26c9d8a025ebf484bcaa701d964")
graph_timeseries(norm_prep, "f3ee6597e842fb1961db2a37c2b306fa")
# Sparse (common)
graph_timeseries(norm_prep, "e3d71385212ccb24cfb1987fe9419e0f")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = "show")
knitr::opts_chunk$set(message = FALSE)
# Clear working directory, load in packages, generate package info
rm(list=ls())
library("tidyverse")
library("trelliscopejs")
sessionInfo()
# Count data
count_normalized <- readRDS("../data_intermediate/SFA2_count_normalized.rds") %>%
gather(Sample, norm_abund, -ASV) %>%
mutate(Sample = as.double(gsub("sa", "", Sample)))
# Taxonomy
tax <- readRDS("../data_intermediate/SFA2_tax_processed.rds") %>%
rownames_to_column(var="ASV")
# Metadata
meta <- read_tsv("../data_amplicon/SFA2_full/SFA2_metadata.tsv")
# Merge
norm_tax_meta <- inner_join(count_normalized, tax, by="ASV") %>%
inner_join(meta, by="Sample") %>%
select(c(Sample, Type:Replicate, Domain:Species, ASV, norm_abund)) %>%
mutate(Innoculant = as_factor(Innoculant),
Replicate = as_factor(Replicate)) %>%
mutate(across(Domain:Species, ~as.character(.x), {.col}))
head(count_normalized)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = "show")
knitr::opts_chunk$set(message = FALSE)
# Clear working directory, load in packages, generate package info
rm(list=ls())
library("tidyverse")
library("ape")
library("phyloseq")
sessionInfo()
# Rep 1
count_rep1 <- read_tsv(file="~/SFAgrowthrate/data_amplicon/SFA2_full/rep1/final/SFA2_rep1.counts-final.tsv")
tax_rep1 <- read_tsv(file="~/SFAgrowthrate/data_amplicon/SFA2_full/rep1/final/SFA2_rep1.taxonomy-final.tsv")
# Rep 2
count_rep2 <- read_tsv(file="~/SFAgrowthrate/data_amplicon/SFA2_full/rep2/final/SFA2_rep2.counts-final.tsv")
tax_rep2 <- read_tsv(file="~/SFAgrowthrate/data_amplicon/SFA2_full/rep2/final/SFA2_rep2.taxonomy-final.tsv")
# Rep 3
count_rep3 <- read_tsv(file="~/SFAgrowthrate/data_amplicon/SFA2_full/rep3/final/SFA2_rep3.counts-final.tsv")
tax_rep3 <- read_tsv(file="~/SFAgrowthrate/data_amplicon/SFA2_full/rep3/final/SFA2_rep3.taxonomy-final.tsv")
# Tree
tree <- read.tree(file="~/SFAgrowthrate/data_amplicon/SFA2_full/tree/SFA2_full.tree-final.nwk")
# Metadata
meta <- read_tsv(file = "~/SFAgrowthrate/data_amplicon/SFA2_full/SFA2_metadata.tsv")
count <- full_join(count_rep1, count_rep2, by="ASV") %>%
full_join(count_rep3, by="ASV")
tax <- full_join(tax_rep1, tax_rep2) %>%
full_join(tax_rep3)
# Reformat and import count table
count_matrix <- count %>%
column_to_rownames(var="ASV") %>%
rename_with(~ gsub("(.+)", "sa\\1", .x)) %>%
as.matrix()
phy_count <- otu_table(count_matrix, taxa_are_rows=TRUE)
# Reformat and import taxonomy table
tax_matrix <- tax %>%
column_to_rownames(var="ASV") %>%
as.matrix()
phy_tax <- tax_table(tax_matrix)
# Metadata import
phy_meta <- sample_data(meta)
# Create phyloseq object
physeq <- phyloseq(phy_count, phy_tax, phy_meta, tree)
physeq
# Remove non-prokaryotic ASVs
physeq_remove_asvs <- taxa_names(subset_taxa(physeq, Order=="Chloroplast" | Family=="Mitochondria" |
Domain=="Unassigned" | Domain=="putative Unassigned" |
Domain=="putative Bacteria"))
physeq_all_asvs <- taxa_names(physeq)
physeq_keep_asvs <- physeq_all_asvs[!(physeq_all_asvs %in% physeq_remove_asvs)]
physeq2 <- prune_taxa(physeq_keep_asvs, physeq)
physeq2
# Filter out ASVs that appear in less than three samples
sparce_pass_asvs <- count %>%
mutate(across(-ASV, ~if_else(.x > 0, 1, 0), {.cols})) %>%
mutate(total = rowSums(.[-1])) %>%
filter(total > 2) %>%
.$ASV
physeq3 <- prune_taxa(sparce_pass_asvs, physeq2)
physeq3
# Extract count and metadata from processed phyloseq object
count_processed <- data.frame(otu_table(physeq3))
meta_processed <- data.frame(sample_data(physeq3)) %>%
select(-Sample) %>%
rownames_to_column(var="Sample")
# Calculate read depth per sample
read_depth <- count_processed %>%
rownames_to_column(var="ASV") %>%
gather(Sample, count, -ASV) %>%
spread(ASV, count) %>%
mutate(total = rowSums(.[-1])) %>%
select(Sample, total) %>%
inner_join(meta_processed)
# Visualize
read_depth %>%
mutate(Replicate = as_factor(Replicate)) %>%
filter(Type != "pcr_control") %>%
ggplot(aes(x=reorder(Sample, -total), y=total, fill=Replicate)) +
geom_col() +
labs(x="Sample", y="Reads") +
theme_test()
# Extract taxonomy table from phyloseq
tax_processed <- data.frame(tax_table(physeq3))
# Isolate Aquifex ASV
aquifex <- tax_processed %>%
filter(Genus == "Aquifex") %>%
rownames_to_column(var="ASV") %>%
select(ASV) %>%
.$ASV
aquifex
# Internal standard read count
total_std <- count_processed %>%
rownames_to_column(var="ASV") %>%
filter(ASV == "445e8681c3c1a735760e6c394f5f4d0a") %>%
select(-ASV) %>%
gather(variable, value) %>%
rename(Sample = variable, std_count = value) %>%
full_join(read_depth, by="Sample")
# ASV richness
richness <- count_processed %>%
mutate(across(everything(), ~ if_else(.x > 0, 1, 0), {.cols})) %>%
colSums() %>%
data.frame() %>%
rownames_to_column(var="Sample")
# Combine
std_depth_rich <- inner_join(total_std, richness) %>%
select(c(Sample, Type:PCR_plate, read_depth=total, std_count, richness=`.`)) %>%
mutate(Replicate = as_factor(Replicate))
# Visualize
std_depth_rich %>%
ggplot(aes(x=Day, y=std_count, color=Replicate)) +
geom_point() +
labs(y="Aquifex ASV reads") +
theme_test()
std_depth_rich %>%
ggplot(aes(x=Day, y=std_count, color=Replicate)) +
geom_point() +
geom_label(aes(label=Sample)) +
labs(y="Aquifex ASV reads") +
theme_test()
# log
std_depth_rich %>%
ggplot(aes(x=Day, y=log(std_count), color=Replicate)) +
geom_point() +
geom_label(aes(label=Sample)) +
labs(y="Aquifex ASV reads") +
theme_test()
# Filter samples with no internal standard reads
redo_nostd <- std_depth_rich %>%
filter(std_count == 0) %>%
.$Sample
redo_nostd
length(redo_nostd)
# Visualize
std_depth_rich %>%
ggplot(aes(x=read_depth, y=std_count, color=Replicate)) +
geom_point() +
#ylim(limits=c(0,500)) +
theme_test()
# Proportion of internal standard
std_depth_rich %>%
mutate(std_prop = std_count/read_depth) %>%
ggplot(aes(y=std_prop, x=Day, color=Replicate)) +
geom_point() +
theme_test()
# Zoom in
std_depth_rich %>%
mutate(std_prop = std_count/read_depth) %>%
ggplot(aes(y=std_prop, x=Day, color=Replicate)) +
geom_point() +
ylim(0, 0.005) +
theme_test()
# Histogram of internal standard reads
std_depth_rich %>%
ggplot() +
geom_histogram(aes(x=std_count)) +
xlim(20,50) +
theme_test()
redo_lowstd <- std_depth_rich %>%
mutate(std_prop = std_count/read_depth) %>%
filter(std_count < 30 & std_count > 0) %>%
.$Sample
redo_lowstd
length(redo_lowstd)
std_depth_rich %>%
mutate(Replicate = as_factor(Replicate)) %>%
ggplot(aes(x=Sample, y=read_depth, color=Replicate)) +
geom_point() +
labs(y="Read depth") +
theme_test()
std_depth_rich %>%
ggplot(aes(x=Sample, y=richness)) +
geom_point() +
labs(y="Total ASVs") +
theme_test()
std_depth_rich %>%
ggplot(aes(x=read_depth, y=richness, color=Replicate)) +
geom_point() +
labs(y="Total ASVs", x="Read depth") +
theme_test()
lowdepth_rich <- std_depth_rich %>%
filter(read_depth < 5000) %>%
filter(Replicate != 1) # remove outlying replicate
highdepth_rich <- std_depth_rich %>%
filter(read_depth > 5000) %>%
filter(Replicate != 1) # remove outlying replicate
mean(lowdepth_rich$richness)
mean(highdepth_rich$richness)
lowdepth_rich <- std_depth_rich %>%
filter(read_depth < 2500) %>%
filter(Replicate != 1) # remove outlying replicate
highdepth_rich <- std_depth_rich %>%
filter(read_depth > 2500) %>%
filter(Replicate != 1) # remove outlying replicate
mean(lowdepth_rich$richness)
mean(highdepth_rich$richness)
lowdepth_rich <- std_depth_rich %>%
filter(read_depth < 2000) %>%
filter(Replicate != 1) # remove outlying replicate
highdepth_rich <- std_depth_rich %>%
filter(read_depth > 2000) %>%
filter(Replicate != 1) # remove outlying replicate
mean(lowdepth_rich$richness)
mean(highdepth_rich$richness)
lowdepth_rich <- std_depth_rich %>%
filter(read_depth < 1500) %>%
filter(Replicate != 1) # remove outlying replicate
highdepth_rich <- std_depth_rich %>%
filter(read_depth > 1500) %>%
filter(Replicate != 1) # remove outlying replicate
mean(lowdepth_rich$richness)
mean(highdepth_rich$richness)
lowdepth_rich <- std_depth_rich %>%
filter(read_depth < 1000) %>%
filter(Replicate != 1) # remove outlying replicate
highdepth_rich <- std_depth_rich %>%
filter(read_depth > 1000) %>%
filter(Replicate != 1) # remove outlying replicate
mean(lowdepth_rich$richness)
mean(highdepth_rich$richness)
redo_lowdepth <- std_depth_rich %>%
filter(read_depth < 3000) %>%
.$Sample
redo_lowdepth
length(redo_lowdepth)
# Count non-missing ASVs per sample
present <- count_processed %>%
mutate(across(everything(), ~if_else(.x == 0, 0, 1))) %>%
colSums() %>%
data.frame() %>%
rownames_to_column(var="Sample") %>%
select(c(Sample, total_asvs = `.`))
# Count ASVs with greater than 30 counts
greater30 <- count_processed %>%
mutate(across(everything(), ~if_else(.x > 30, 1, 0))) %>%
colSums() %>%
data.frame() %>%
rownames_to_column(var="Sample") %>%
select(c(Sample, over_30count = `.`))
# Calculate proportion of ASVs below 30 counts in each sample
asv_reads <- full_join(present, greater30, by="Sample") %>%
full_join(read_depth, by="Sample") %>%
mutate(prop = over_30count/total_asvs)
asv_reads %>%
ggplot() +
geom_histogram(aes(x=prop)) +
labs(x="Proportion of ASVs with more than 30 counts") +
theme_test()
asv_reads %>%
ggplot() +
geom_point(aes(x=total, y=prop)) +
labs(x="Proportion of ASVs with more than 30 counts") +
theme_test()
asv_reads %>%
ggplot() +
geom_point(aes(y=over_30count, x=total)) +
labs(y="ASVs with over 30 reads", x="Read depth") +
theme_test()
# 10%
asv_reads %>%
filter(prop < 0.1) %>%
nrow()
# 20%
asv_reads %>%
filter(prop < 0.2) %>%
nrow()
redo_samples <- unique(c(redo_nostd, redo_lowstd, redo_lowdepth, "sa383"))
redo_samples
length(redo_samples)
# Normalize
count_normalized<- count_processed %>%
rownames_to_column(var="ASV") %>%
gather(Sample, value, -ASV) %>%
spread(ASV, value) %>%
mutate(across(-Sample, ~ .x/`445e8681c3c1a735760e6c394f5f4d0a`, {.col})) # Divide ASV counts by Aquifex ASV counts
head(count_normalized)
count_normalized
View(count_normalized)
# ASV counts
# Reformat
count_normalized2 <- count_normalized %>%
gather(ASV, value, -Sample) %>%
spread(Sample, value)
count_normalized2
# Count data
count_normalized <- readRDS("../data_intermediate/SFA2_count_normalized.rds") %>%
gather(Sample, ARNIS_ratio, -ASV) %>%
mutate(Sample = as.double(gsub("sa", "", Sample)))
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = "show")
knitr::opts_chunk$set(message = FALSE)
# Clear working directory, load in packages, generate package info
rm(list=ls())
library("tidyverse")
library("trelliscopejs")
sessionInfo()
# Count data
count_normalized <- readRDS("../data_intermediate/SFA2_count_normalized.rds") %>%
gather(Sample, ARNIS_ratio, -ASV) %>%
mutate(Sample = as.double(gsub("sa", "", Sample)))
head(count_normalized)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = "show")
knitr::opts_chunk$set(message = FALSE)
# Clear working directory, load in packages, generate package info
rm(list=ls())
library("tidyverse")
library("trelliscopejs")
sessionInfo()
# Count data
count_normalized <- readRDS("../data_intermediate/SFA2_count_normalized.rds") %>%
gather(Sample, ARNIS_ratio, -ASV) %>%
mutate(Sample = as.double(gsub("sa", "", Sample)))
# Taxonomy
tax <- readRDS("../data_intermediate/SFA2_tax_processed.rds") %>%
rownames_to_column(var="ASV")
# Metadata
meta <- read_tsv("../data_amplicon/SFA2_full/SFA2_metadata.tsv")
# Merge
norm_tax_meta <- inner_join(count_normalized, tax, by="ASV") %>%
inner_join(meta, by="Sample") %>%
select(c(Sample, Type:Replicate, Domain:Species, ASV, ARNIS_ratio)) %>%
mutate(Innoculant = as_factor(Innoculant),
Replicate = as_factor(Replicate)) %>%
mutate(across(Domain:Species, ~as.character(.x), {.col}))
norm_prep <- norm_tax_meta %>%
filter(ASV != "445e8681c3c1a735760e6c394f5f4d0a")
norm_prep <- norm_prep[!is.infinite(norm_prep$ARNIS_ratio),]
norm_prep <- norm_prep %>%
filter(ARNIS_ratio != 0)
norm_prep <- norm_prep %>%
group_by(Type, Innoculant, DOC_pheno, Day, Domain, Phylum, Class, Order, Family, Genus, Species, ASV) %>%
summarize(ARNIS_ratio_avg = mean(ARNIS_ratio, na.rm = TRUE))
# Identify ASVs with less than 4 time points
norm_occurs <- norm_prep %>%
group_by(ASV, Type, Innoculant) %>%
summarize(occurs = n()) %>%
filter(occurs > 3)
# Filter
norm_prep <- inner_join(norm_prep, norm_occurs, by=c("ASV", "Innoculant", "Type")) %>%
select(everything(), -occurs)
norm_prep <- norm_prep %>%
mutate(ln_abund = log(ARNIS_ratio_avg))
# Slow
# Opens in browser
norm_prep %>%
filter(Type=="Growth") %>%
ggplot(aes(x=Day, y=ln_abund, color=Innoculant)) +
facet_wrap(~DOC_pheno) +
geom_point() +
geom_line() +
facet_trelliscope(~ASV, scales="free_y", path="rmarkdown_files", self_contained=TRUE, height = 400, width = 1600) +
labs(y="Normalized abundance (ln)", x="Day") +
theme_test()
# Function to extract taxonomic info for graphs
extract_tax <- function(df, asv, level) { # Data frame, "ASV", "Taxonomy level"
temp <- df %>%
filter(ASV == asv)
tax <- unique(temp[[level]])
return(tax)
}
# Time series graphing function
graph_timeseries <- function(df, asv) {
graph <- df %>%
filter(ASV==asv) %>%
ggplot(aes(x=Day, y=ln_abund, color=Innoculant)) +
facet_wrap(~DOC_pheno) +
geom_point() +
geom_line() +
labs(y="Normalized abundance (ln)", x="Day", title=paste0(extract_tax(norm_prep, asv, "Phylum"), ", ",
extract_tax(norm_prep, asv, "Genus"))) +
theme_test()
return(graph)
}
# Graph
# Robust growth?
graph_timeseries(norm_prep, "fed2377b60ef09790fc532bbbe2b4602")
graph_timeseries(norm_prep, "fa9bfdf1098ec99a73f104f85e5495fb")
graph_timeseries(norm_prep, "b395f80b4600c403b0f6e8164d0b0426")
# Death
graph_timeseries(norm_prep, "0d91c5a1f756f5e13a32d818129c3311")
graph_timeseries(norm_prep, "13e479df5562e8e93d620ac363eb9644")
# Low abundance issues?
graph_timeseries(norm_prep, "0924a26c9d8a025ebf484bcaa701d964")
graph_timeseries(norm_prep, "f3ee6597e842fb1961db2a37c2b306fa")
# Sparse (common)
graph_timeseries(norm_prep, "e3d71385212ccb24cfb1987fe9419e0f")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = "show")
knitr::opts_chunk$set(message = FALSE)
# Clear working directory, load in packages, generate package info
rm(list=ls())
library("tidyverse")
library("trelliscopejs")
sessionInfo()
# Count data
count_normalized <- readRDS("../data_intermediate/SFA2_count_normalized.rds") %>%
gather(Sample, ARNIS_ratio, -ASV) %>%
mutate(Sample = as.double(gsub("sa", "", Sample)))
# Taxonomy
tax <- readRDS("../data_intermediate/SFA2_tax_processed.rds") %>%
rownames_to_column(var="ASV")
# Metadata
meta <- read_tsv("../data_amplicon/SFA2_full/SFA2_metadata.tsv")
# Merge
norm_tax_meta <- inner_join(count_normalized, tax, by="ASV") %>%
inner_join(meta, by="Sample") %>%
select(c(Sample, Type:Replicate, Domain:Species, ASV, ARNIS_ratio)) %>%
mutate(Innoculant = as_factor(Innoculant),
Replicate = as_factor(Replicate)) %>%
mutate(across(Domain:Species, ~as.character(.x), {.col}))
norm_prep <- norm_tax_meta %>%
filter(ASV != "445e8681c3c1a735760e6c394f5f4d0a")
norm_prep <- norm_prep[!is.infinite(norm_prep$ARNIS_ratio),]
norm_prep <- norm_prep %>%
filter(ARNIS_ratio != 0)
norm_prep <- norm_prep %>%
group_by(Type, Innoculant, DOC_pheno, Day, Domain, Phylum, Class, Order, Family, Genus, Species, ASV) %>%
summarize(ARNIS_ratio_avg = mean(ARNIS_ratio, na.rm = TRUE))
# Identify ASVs with less than 4 time points
norm_occurs <- norm_prep %>%
group_by(ASV, Type, Innoculant) %>%
summarize(occurs = n()) %>%
filter(occurs > 3)
# Filter
norm_prep <- inner_join(norm_prep, norm_occurs, by=c("ASV", "Innoculant", "Type")) %>%
select(everything(), -occurs)
norm_prep <- norm_prep %>%
mutate(ln_abund = log(ARNIS_ratio_avg))
# Function to extract taxonomic info for graphs
extract_tax <- function(df, asv, level) { # Data frame, "ASV", "Taxonomy level"
temp <- df %>%
filter(ASV == asv)
tax <- unique(temp[[level]])
return(tax)
}
# Time series graphing function
graph_timeseries <- function(df, asv) {
graph <- df %>%
filter(ASV==asv) %>%
ggplot(aes(x=Day, y=ln_abund, color=Innoculant)) +
facet_wrap(~DOC_pheno) +
geom_point() +
geom_line() +
labs(y="ln ARNIS ratio", x="Day", title=paste0(extract_tax(norm_prep, asv, "Phylum"), ", ",
extract_tax(norm_prep, asv, "Genus"))) +
theme_test()
return(graph)
}
# Graph
# Robust growth?
graph_timeseries(norm_prep, "fed2377b60ef09790fc532bbbe2b4602")
graph_timeseries(norm_prep, "fa9bfdf1098ec99a73f104f85e5495fb")
graph_timeseries(norm_prep, "b395f80b4600c403b0f6e8164d0b0426")
# Death
graph_timeseries(norm_prep, "0d91c5a1f756f5e13a32d818129c3311")
graph_timeseries(norm_prep, "13e479df5562e8e93d620ac363eb9644")
# Low abundance issues?
graph_timeseries(norm_prep, "0924a26c9d8a025ebf484bcaa701d964")
graph_timeseries(norm_prep, "f3ee6597e842fb1961db2a37c2b306fa")
head(norm_prep)
norm_prep <- norm_prep %>%
mutate(label = paste0(Type, Innoculant, "_", ASV))
head(norm_prep)
head(norm_prep)
?inner_join
?select
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = "show")
knitr::opts_chunk$set(message = FALSE)
# Clear working directory, load in packages, generate package info
rm(list=ls())
library("tidyverse")
sessionInfo()
# Rep 1
count_rep1 <- read_tsv(file="../data_amplicon/SFA2_full/rep1/final/SFA2_rep1.counts-final.tsv")
tax_rep1 <- read_tsv(file="../data_amplicon/SFA2_full/rep1/final/SFA2_rep1.taxonomy-final.tsv")
# Rep 2
count_rep2 <- read_tsv(file="../data_amplicon/SFA2_full/rep2/final/SFA2_rep2.counts-final.tsv")
tax_rep2 <- read_tsv(file="../data_amplicon/SFA2_full/rep2/final/SFA2_rep2.taxonomy-final.tsv")
# Rep 3
count_rep3 <- read_tsv(file="../data_amplicon/SFA2_full/rep3/final/SFA2_rep3.counts-final.tsv")
tax_rep3 <- read_tsv(file="../data_amplicon/SFA2_full/rep3/final/SFA2_rep3.taxonomy-final.tsv")
# Metadata
meta <- read_tsv(file = "../data_amplicon/SFA2_full/SFA2_metadata.tsv")
