# If a suitable fit is found and more time points exist that were not included, try extending the window
if (window_p <= 0.05 & window_coeff > 0 & end < nrow(df_sub)) {
for (extend_end in ((end+1):nrow(df_sub))) {
if (stop == TRUE) {break}
# Fit linear model to previous, non-extended window
prevwindow_lm <- NULL; prevwindow_p <- NULL; prevwindow_coeff <- NULL
prevwindow_lm <- lm(abund ~ time, data = df_sub[start:(extend_end - 1),])
prevwindow_p <- summary(prevwindow_lm)$coefficients[2,4]
prevwindow_coeff <- prevwindow_lm$coefficients[2]
# Fit linear model to the extended window
newwindow_lm <- NULL; newwindow_p <- NULL; newwindow_coeff <- NULL
newwindow_lm <- lm(abund ~ time, data = df_sub[start:extend_end,])
newwindow_p <- summary(newwindow_lm)$coefficients[2,4]
newwindow_coeff <- newwindow_lm$coefficients[2]
# If see improvement and can add more data, continue extending the window
if (newwindow_p <= prevwindow_p & newwindow_coeff > 0 & extend_end < nrow(df_sub)) {
next
}
# If no improvement, save the previous fit
else if (newwindow_p > prevwindow_p & prevwindow_coeff > 0 & extend_end < nrow(df_sub)) {
end <- extend_end - 1
sim_estimates <- save_fit(label, start, end, df_sub, sim_estimates)
stop <- TRUE
}
# If see improvement (or no harm) but no more data points to fit, save the extended fit
else if (newwindow_p <= prevwindow_p & newwindow_coeff > 0 & extend_end == nrow(df_sub)) {
end <- extend_end
sim_estimates <- save_fit(label, start, end, df_sub, sim_estimates)
stop <- TRUE
}
}
}
# If no more data available to add to model, save the fit
else if (window_p <= 0.05 & window_coeff > 0 & end == nrow(df_sub)) {
sim_estimates <- save_fit(label, start, end, df_sub, sim_estimates)
stop <- TRUE
}
}
}
}
dim(sim_estimates)
# Remove "perfect" fits as precaution
sim_estimates <- filter(sim_estimates, residuals >= 0.0001)
dim(sim_estimates)
# Choose lowest p-value window
sim_lowest_pvals <- sim_estimates %>%
group_by(label) %>%
summarize(pval = min(pval)) %>%
ungroup()
dim(sim_lowest_pvals)
# Filter chosen estimates
sim_estimates <- sim_estimates %>%
semi_join(sim_lowest_pvals)
dim(sim_estimates)
# False positives
a <- nrow(sim_estimates[sim_estimates$pval <= 0.05,])
b <- nrow(sim_estimates[sim_estimates$pval <= 0.025,])
c <- nrow(sim_estimates[sim_estimates$pval <= 0.01,])
d <- nrow(sim_estimates[sim_estimates$pval <= 0.005,])
e <- nrow(sim_estimates[sim_estimates$pval <= 0.001,])
f <- nrow(sim_estimates[sim_estimates$pval <= 0.0005,])
false_pos <- data.frame(c(0.05, 0.025, 0.01, 0.005, 0.001, 0.0005), c(a,b,c,d,e,f))
colnames(false_pos)=c("pvalue","false")
ggplot(false_pos, aes(x=pvalue, y=false)) +
geom_point() +
geom_smooth(method="lm", linetype=2) +
labs(title="Relationship between p-value and number of false positives", x="P-value", y="False positives") +
theme_test()
falsepos_lm <- lm(false ~ pvalue, data=false_pos)
falsepos_lm
# Extract model coefficients
slope <- as.numeric(falsepos_lm$coefficients[2])
intercept <- as.numeric(falsepos_lm$coefficients[1])
# 10% false positive allowance
false10_pval <- (100 - intercept)/slope
## 5% false positive allowance
false5_pval <- (50 - intercept)/slope
# 2.5% false positive allowance
false2.5_pval <- (25 - intercept)/slope
# 1% false positive allowance
false1_pval <- (10 - intercept)/slope
# ~10% false positives
growth_falsepos10 <- subset(growth_estimates, pval <= false10_pval)
nrow(growth_falsepos10)
# ~5% false positives
growth_falsepos5 <- subset(growth_estimates, pval <= false5_pval)
nrow(growth_falsepos5)
# 2.5%
growth_falsepos2.5 <- subset(growth_estimates, pval <= false2.5_pval)
nrow(growth_falsepos2.5)
# 1%
growth_falsepos1 <- subset(growth_estimates, pval <= false1_pval)
nrow(growth_falsepos1)
set.seed(10)
# 10% false positive rate
labels10 <- as.character(growth_falsepos10$label)
# Randomly plot 10 w/growth estimate
for (n in 1:10) {
# Choose randomly
rand_num <- sample(1:nrow(growth_falsepos10), 1)
l <- labels10[rand_num]
# Estimated growth window
start <- growth_falsepos10 %>%
filter(label==l) %>%
.$start
end <- growth_falsepos10 %>%
filter(label==l) %>%
.$end
# Plot
data_sub <- norm_prepped %>%
filter(label == l)
plot <- data_sub %>%
ggplot(aes(x=time, y=abund)) +
geom_smooth(method="lm", data=data_sub[start:end,], linetype=2) +
geom_point() +
geom_line() +
theme_test()
print(plot)
}
set.seed(5)
# 10% false positive rate
labels5 <- as.character(growth_falsepos5$label)
# Randomly plot 10 w/growth estimate
for (n in 1:10) {
# Choose randomly
rand_num <- sample(1:nrow(growth_falsepos5), 1)
l <- labels5[rand_num]
# Estimated growth window
start <- growth_falsepos5 %>%
filter(label==l) %>%
.$start
end <- growth_falsepos10 %>%
filter(label==l) %>%
.$end
# Plot
data_sub <- norm_prepped %>%
filter(label == l)
plot <- data_sub %>%
ggplot(aes(x=time, y=abund)) +
geom_smooth(method="lm", data=data_sub[start:end,], linetype=2) +
geom_point() +
geom_line() +
theme_test()
print(plot)
}
# Generate abundances over time interval based on estimated slope
growth_estimates <- data.frame()
B <- 1 # Start with one bacteria
for (l in growth_falsepos10$label) {
growth_label <- filter(growth_falsepos10, label==l)
slope <- growth_label$coeff
b <- (slope*3)+1 # abundance three days later
k <- (log10(b)-log10(B))*(2.303/3) # calculate k
this_row <- cbind(growth_label, k)
growth_estimates <- rbind(growth_estimates, this_row)
}
# Convert start and end to actual day
growth_final <- data.frame()
for (l in as.character(unique(growth_estimates$label))) {
# Isolate timeseries
norm_label <- norm_prepped %>%
filter(label==l) %>%
arrange(time)
growth_label <- filter(growth_estimates, label==l)
start <- growth_label$start
end <- growth_label$end
# Start and end day of growth
start_day <- norm_label[start,]$time
end_day <- norm_label[end,]$time
# Starting and ending relational abundance
start_abund <- norm_label[start,]$norm_abund_avg
end_abund <- norm_label[end,]$norm_abund_avg
change_abund <- end_abund - start_abund
# Save output
this_row <- bind_cols(label = as.character(growth_label$label), k = growth_label$k,
start_pt = growth_label$start, end_pt = growth_label$end,
start_day = start_day, end_day = end_day,
start_abund = start_abund, end_abund = end_abund, change_abund = change_abund)
growth_final <- bind_rows(growth_final, this_row)
}
head(growth_final)
# Calculate doubling time based on k
g.df <- data.frame()
for (l in as.character(unique(growth_final$label))) {
data <- data.frame()
g <- NULL
k <- NULL
data_sub <- growth_final[label==l,]
k <- data_sub$k
g <- log(2)/k
thisrow <- cbind(data_sub, g)
g.df <- rbind(g.df, thisrow)
}
g.df
data_sub
growht_final
growth_final
l
# Calculate doubling time based on k
g.df <- data.frame()
for (l in as.character(unique(growth_final$label))) {
data_sub <- growth_final[label==l,]
k <- data_sub$k
g <- log(2)/k
thisrow <- cbind(data_sub, g)
g.df <- rbind(g.df, thisrow)
}
g.df
k
data_sub
l
data_sub <- filter(growth_final, label==l)
k <- data_sub$k
k
# Calculate doubling time based on k
g.df <- data.frame()
for (l in as.character(unique(growth_final$label))) {
data_sub <- filter(growth_final, label==l)
k <- data_sub$k
g <- log(2)/k
thisrow <- cbind(data_sub, g)
g.df <- rbind(g.df, thisrow)
}
g.df
mean(g.df$g)
mean(g.df$g)
sd(g.df$g)
min(g.df$g)
max(g.df$g)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = "show")
knitr::opts_chunk$set(message = FALSE)
# Clear working directory, load in packages, generate package info
rm(list=ls())
library("tidyverse")
library("trelliscopejs")
sessionInfo()
# Relational abundances, prepped
norm_prepped <- readRDS("../data_intermediate/SFA2_norm_prepped.rds")
# Taxonomy
tax <- readRDS("../data_intermediate/SFA2_taxonomy.rds")
# Metadata
meta <- readRDS("../data_intermediate/SFA2_metadata.rds")
# Function: Saves chosen estimate from below algorhithm
save_fit <- function(label, start, end, df_sub, output) {
# Save estimate info
est <- NULL; coeff <- NULL; residuals <- NULL; pval <- NULL; thisrow <- data.frame() # clear previous
est <- lm(abund ~ time, data=df_sub[start:end,])
coeff <- as.numeric(est$coefficients[2])
residuals <- sum(abs(resid(est)))
pval <- summary(est)$coefficients[2,4]
thisrow <- data.frame(label, start, end, coeff, pval, residuals)
output <- bind_rows(output, thisrow)
return(output)
}
## Requires a dataframe with columns containing a unique label for each data point, abundance values, time points
## df = data frame containing time series with abundance values (ln transformed), long format
## df must contain columns named:
### label = column with unique identifier for each time series
### abund = column with abundance values at each time point
### time = column with time point values
df <- norm_prepped
death_estimates <- data.frame()
for (label in as.character(unique(df$label))) {
# Subset one time series using the label
df_sub <- data.frame()
df_sub <- df[df$label==label,]
stop <- FALSE
# Sliding window
for (start in 1:(nrow(df_sub) - 3)) {
stop <- FALSE
for (end in (start + 3):nrow(df_sub)) {
if (stop == TRUE) {break}
# Fit linear model to the window
window_lm <- NULL; window_p <- NULL; window_coeff <- NULL
window_lm <- lm(abund ~ time, data = df_sub[start:end,])
window_p <- summary(window_lm)$coefficients[2,4]
window_coeff <- window_lm$coefficients[2]
# If a suitable fit is found and more time points exist that were not included, try extending the window
if (window_p <= 0.05 & window_coeff < 0 & end < nrow(df_sub)) {
for (extend_end in ((end+1):nrow(df_sub))) {
if (stop == TRUE) {break}
# Fit linear model to previous, non-extended window
prevwindow_lm <- NULL; prevwindow_p <- NULL; prevwindow_coeff <- NULL
prevwindow_lm <- lm(abund ~ time, data = df_sub[start:(extend_end - 1),])
prevwindow_p <- summary(prevwindow_lm)$coefficients[2,4]
prevwindow_coeff <- prevwindow_lm$coefficients[2]
# Fit linear model to the extended window
newwindow_lm <- NULL; newwindow_p <- NULL; newwindow_coeff <- NULL
newwindow_lm <- lm(abund ~ time, data = df_sub[start:extend_end,])
newwindow_p <- summary(newwindow_lm)$coefficients[2,4]
newwindow_coeff <- newwindow_lm$coefficients[2]
# If see improvement and can add more data, continue extending the window
if (newwindow_p <= prevwindow_p & newwindow_coeff < 0 & extend_end < nrow(df_sub)) {
next
}
# If no improvement, save the previous fit
else if (newwindow_p > prevwindow_p & prevwindow_coeff < 0 & extend_end < nrow(df_sub)) {
end <- extend_end - 1
death_estimates <- save_fit(label, start, end, df_sub, death_estimates)
stop <- TRUE
}
# If see improvement (or no harm) but no more data points to fit, save the extended fit
else if (newwindow_p <= prevwindow_p & newwindow_coeff < 0 & extend_end == nrow(df_sub)) {
end <- extend_end
death_estimates <- save_fit(label, start, end, df_sub, death_estimates)
stop <- TRUE
}
}
}
# If no more data available to add to model, save the fit
else if (window_p <= 0.05 & window_coeff < 0 & end == nrow(df_sub)) {
death_estimates <- save_fit(label, start, end, df_sub, death_estimates)
stop <- TRUE
}
}
}
}
dim(death_estimates)
# Remove "perfect" fits as precaution
death_estimates <- filter(death_estimates, residuals >= 0.0001)
dim(death_estimates)
# Choose lowest p-value window
lowest_pvals <- death_estimates %>%
group_by(label) %>%
summarize(pval = min(pval)) %>%
ungroup()
dim(lowest_pvals)
# Filter chosen estimates
death_estimates <- death_estimates %>%
semi_join(lowest_pvals)
dim(death_estimates)
hist(death_estimates$pval, xlab="P-values", main="Histogram of quality filtered p-values")
sim_data <- readRDS("../data_intermediate/SFA2_simulated.rds")
# Prep simulated data (rename columns)
sim_prepped <- sim_data %>%
select(label=simulation, abund=rand_abund, time=rand_day)
# Run algorithinmmmnmnm
df <- sim_prepped
sim_estimates <- data.frame()
for (label in as.character(unique(df$label))) {
# Subset one time series using the label
df_sub <- data.frame()
df_sub <- df[df$label==label,]
stop <- FALSE
# Sliding window
for (start in 1:(nrow(df_sub) - 3)) {
stop <- FALSE
for (end in (start + 3):nrow(df_sub)) {
if (stop == TRUE) {break}
# Fit linear model to the window
window_lm <- NULL; window_p <- NULL; window_coeff <- NULL
window_lm <- lm(abund ~ time, data = df_sub[start:end,])
window_p <- summary(window_lm)$coefficients[2,4]
window_coeff <- window_lm$coefficients[2]
# If a suitable fit is found and more time points exist that were not included, try extending the window
if (window_p <= 0.05 & window_coeff < 0 & end < nrow(df_sub)) {
for (extend_end in ((end+1):nrow(df_sub))) {
if (stop == TRUE) {break}
# Fit linear model to previous, non-extended window
prevwindow_lm <- NULL; prevwindow_p <- NULL; prevwindow_coeff <- NULL
prevwindow_lm <- lm(abund ~ time, data = df_sub[start:(extend_end - 1),])
prevwindow_p <- summary(prevwindow_lm)$coefficients[2,4]
prevwindow_coeff <- prevwindow_lm$coefficients[2]
# Fit linear model to the extended window
newwindow_lm <- NULL; newwindow_p <- NULL; newwindow_coeff <- NULL
newwindow_lm <- lm(abund ~ time, data = df_sub[start:extend_end,])
newwindow_p <- summary(newwindow_lm)$coefficients[2,4]
newwindow_coeff <- newwindow_lm$coefficients[2]
# If see improvement and can add more data, continue extending the window
if (newwindow_p <= prevwindow_p & newwindow_coeff < 0 & extend_end < nrow(df_sub)) {
next
}
# If no improvement, save the previous fit
else if (newwindow_p > prevwindow_p & prevwindow_coeff < 0 & extend_end < nrow(df_sub)) {
end <- extend_end - 1
sim_estimates <- save_fit(label, start, end, df_sub, sim_estimates)
stop <- TRUE
}
# If see improvement (or no harm) but no more data points to fit, save the extended fit
else if (newwindow_p <= prevwindow_p & newwindow_coeff < 0 & extend_end == nrow(df_sub)) {
end <- extend_end
sim_estimates <- save_fit(label, start, end, df_sub, sim_estimates)
stop <- TRUE
}
}
}
# If no more data available to add to model, save the fit
else if (window_p <= 0.05 & window_coeff < 0 & end == nrow(df_sub)) {
sim_estimates <- save_fit(label, start, end, df_sub, sim_estimates)
stop <- TRUE
}
}
}
}
dim(sim_estimates)
# Remove "perfect" fits as precaution
sim_estimates <- filter(sim_estimates, residuals >= 0.0001)
dim(sim_estimates)
# Choose lowest p-value window
sim_lowest_pvals <- sim_estimates %>%
group_by(label) %>%
summarize(pval = min(pval)) %>%
ungroup()
dim(sim_lowest_pvals)
# Filter chosen estimates
sim_estimates <- sim_estimates %>%
semi_join(sim_lowest_pvals)
dim(sim_estimates)
# False positives
a <- nrow(sim_estimates[sim_estimates$pval <= 0.05,])
b <- nrow(sim_estimates[sim_estimates$pval <= 0.025,])
c <- nrow(sim_estimates[sim_estimates$pval <= 0.01,])
d <- nrow(sim_estimates[sim_estimates$pval <= 0.005,])
e <- nrow(sim_estimates[sim_estimates$pval <= 0.001,])
f <- nrow(sim_estimates[sim_estimates$pval <= 0.0005,])
false_pos <- data.frame(c(0.05, 0.025, 0.01, 0.005, 0.001, 0.0005), c(a,b,c,d,e,f))
colnames(false_pos)=c("pvalue","false")
ggplot(false_pos, aes(x=pvalue, y=false)) +
geom_point() +
geom_smooth(method="lm") +
theme_test() +
labs(title="Relationship between p-value and number of false positives", x="P-value", y="False positives")
falsepos_lm <- lm(false ~ pvalue, data=false_pos)
falsepos_lm
# Extract model coefficients
slope <- as.numeric(falsepos_lm$coefficients[2])
intercept <- as.numeric(falsepos_lm$coefficients[1])
# 10% false positive allowance
false10_pval <- (100 - intercept)/slope
## 5% false positive allowance
false5_pval <- (50 - intercept)/slope
# 2.5% false positive allowance
false2.5_pval <- (25 - intercept)/slope
# 1% false positive allowance
false1_pval <- (10 - intercept)/slope
# ~10% false positives
death_falsepos10 <- subset(death_estimates, pval <= false10_pval)
nrow(death_falsepos10)
# ~5% false positives
death_falsepos5 <- subset(death_estimates, pval <= false5_pval)
nrow(death_falsepos5)
# 2.5%
death_falsepos2.5 <- subset(death_estimates, pval <= false2.5_pval)
nrow(death_falsepos2.5)
# 1%
death_falsepos1 <- subset(death_estimates, pval <= false1_pval)
nrow(death_falsepos1)
# Convert start and end to actual day
death_final <- data.frame()
for (l in as.character(unique(death_falsepos10$label))) {
# Isolate timeseries
norm_label <- norm_prepped %>%
filter(label==l) %>%
arrange(time)
death_label <- filter(death_falsepos10, label==l)
start <- death_label$start
end <- death_label$end
# Start and end day of death
start_day <- norm_label[start,]$time
end_day <- norm_label[end,]$time
# Starting and ending relational abundance
start_abund <- norm_label[start,]$norm_abund_avg
end_abund <- norm_label[end,]$norm_abund_avg
change_abund <- end_abund - start_abund
# Save output
this_row <- bind_cols(label = as.character(death_label$label), k = death_label$coeff,
start_pt = death_label$start, end_pt = death_label$end,
start_day = start_day, end_day = end_day,
start_abund = start_abund, end_abund = end_abund, change_abund = change_abund)
death_final <- bind_rows(death_final, this_row)
}
# Calculate doubling time based on k
h.df <- data.frame()
for (l in as.character(unique(growth_final$label))) {
data_sub <- filter(growth_final, label==l)
k <- abs(data_sub$k)
h <- log(2)/k
thisrow <- cbind(data_sub, h)
h.df <- rbind(h.df, thisrow)
}
# Calculate doubling time based on k
h.df <- data.frame()
for (l in as.character(unique(death_final$label))) {
data_sub <- filter(death_final, label==l)
k <- abs(data_sub$k)
h <- log(2)/k
thisrow <- cbind(data_sub, h)
h.df <- rbind(h.df, thisrow)
}
h.df
mean(h.df$g)
sd(h.df$g)
min(h.df$g)
max(h.df$g)
head(death_final)
k
# Calculate doubling time based on k
h.df <- data.frame()
for (l in as.character(unique(death_final$label))) {
data_sub <- filter(death_final, label==l)
k <- abs(data_sub$k)
h <- log(2)/k
thisrow <- cbind(data_sub, h)
h.df <- rbind(h.df, thisrow)
}
h.df
head(h.df)
mean(h.df$h)
sd(h.df$h)
min(h.df$h)
max(h.df$h)
