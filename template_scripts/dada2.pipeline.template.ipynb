{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Title\n",
    "\n",
    "Author and date\n",
    "\n",
    "Notes:\n",
    "* Running script using QIIME2 v2019.7\n",
    "* Original script written by Roli Wilhelm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to process raw sequences into phyloseq object with DADA2 ###\n",
    "* Prep for Import to QIIME2  (Combine two index files)\n",
    "* Import to QIIME2\n",
    "* Demultiplex\n",
    "* Denoise and Merge\n",
    "* Prepare OTU Tables and Rep Sequences  *(Note: sample names starting with a digit will break this step)*\n",
    "* Classify Seqs\n",
    "\n",
    "*100% Appropriated from the \"Atacama Desert Tutorial\" for QIIME2*\n",
    "\n",
    "### Pipeline can handle both 16S rRNA gene and ITS sequences####\n",
    "* Tested on 515f and 806r\n",
    "* Tested on ITS1\n",
    "\n",
    "### Commands to install dependencies ####\n",
    "##### || QIIME2 ||\n",
    "** Note: QIIME2 is still actively in development, and I've noticed frequent new releases. Check for the most up-to-date conda install file <https://docs.qiime2.org/2017.11/install/native/#install-qiime-2-within-a-conda-environment>\n",
    "\n",
    "* wget https://data.qiime2.org/distro/core/qiime2-2018.2-py35-linux-conda.yml\n",
    "* conda env create -n qiime2-2018.2 --file qiime2-2018.2-py35-linux-conda.yml\n",
    "* source activate qiime2-pipeline\n",
    "\n",
    "##### || Copyrighter rrn database ||\n",
    "* The script will automatically install the curated GreenGenes rrn attribute database\n",
    "* https://github.com/fangly/AmpliCopyrighter\n",
    "\n",
    "##### || rpy2 (don't use conda version) ||\n",
    "* pip install rpy2  \n",
    "\n",
    "##### || phyloseq ||\n",
    "* conda install -c r r-igraph \n",
    "* Rscript -e \"source('http://bioconductor.org/biocLite.R');biocLite('phyloseq')\" \n",
    "\n",
    "##### || R packages ||\n",
    "* ape   (natively installed in conda environment)\n",
    "\n",
    "### Citations ###\n",
    "* Caporaso, J. G., Kuczynski, J., Stombaugh, J., Bittinger, K., Bushman, F. D., Costello, E. K., *et al.* (2010). QIIME allows analysis of high-throughput community sequencing data. Nature methods, 7(5), 335-336.\n",
    "\n",
    "* McMurdie and Holmes (2013) phyloseq: An R Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data. PLoS ONE. 8(4):e61217\n",
    "\n",
    "* Paradis E., Claude J. & Strimmer K. 2004. APE: analyses of phylogenetics and evolution in R language. Bioinformatics 20: 289-290.\n",
    "\n",
    "* Angly, F. E., Dennis, P. G., Skarshewski, A., Vanwonterghem, I., Hugenholtz, P., & Tyson, G. W. (2014). CopyRighter: a rapid tool for improving the accuracy of microbial community profiles through lineage-specific gene copy number correction. Microbiome, 2(1), 11.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: User Input\n",
    "\n",
    "Notes\n",
    "* The metadata file must be located in the project directory\n",
    "* Metadata file must be .tsv format and have one column named \"BarcodeSequence\" with the relevant primers (rev. comp reverse concatenated with forward primer sequence)\n",
    "* The output directories will be created inside the project directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, numpy as np\n",
    "\n",
    "# Prepare an object with the name of the library and all related file paths\n",
    "# datasets = [['name', 'project directory path', 'output directory name', 'raw data directory', read1 file name, read2 file name, 'metadata file name','domain of life'],...]\n",
    "datasets = [[]]\n",
    "# Notes: \n",
    "# It's no longer possible to install QIIME1. Read1 and Read2 were modified with seqid_truncate.py as a work around\n",
    "# seqid_truncate.py removes a portion of each sequence ID that isn't useful but interferes with demultiplexing\n",
    "# This made the fastq files compatible with our custom barcode extraction script (concatenate_barcodes_mod.py) that replaces QIIME1's (extract_barcodes.py)\n",
    "\n",
    "# Set # of processors to use\n",
    "processors = 10\n",
    "\n",
    "# Which bacterial database will you use? Silva or GreenGenes\n",
    "db = \"Silva\"\n",
    "\n",
    "# Phylogenetic tree (non-fungal data)\n",
    "treepath = \"\" # full path to the phylogenetic tree file to be made\n",
    "treename = \"\" # name prefix for the tree file\n",
    "\n",
    "## Enter minimum support for keeping QIIME classification\n",
    "# Note: Classifications that do not meet this criteria will simply be retained, but labeled 'putative'\n",
    "min_support = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Concatenate barcodes for QIIME2 pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: QIIME takes a single barcode file. The command 'extract_barcodes.py' concatenates the forward and reverse read barcode and attributes it to a single read.\n",
    "\n",
    "# See http://qiime.org/tutorials/processing_illumina_data.html\n",
    "\n",
    "for dataset in datasets:\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    raw = dataset[3]\n",
    "    read1 = dataset[4]\n",
    "    read2 = dataset[5]\n",
    "    #index1 = dataset[6]\n",
    "    #index2 = dataset[7]\n",
    "    \n",
    "    # Run extract_barcodes to merge the two index files\n",
    "    #!python2 /opt/anaconda2/bin/extract_barcodes.py --input_type barcode_paired_end -f $raw/$index1 -r $raw/$index2 --bc1_len 8 --bc2_len 8 -o $directory/$output\n",
    "    \n",
    "    # Note:\n",
    "    # Above (commented out) step is broken because QIIME1 cannot be reinstalled\n",
    "    # Instead, I used concatenate_barcodes_mod.py script outside this pipeline to produce barcodes.fastq\n",
    "    # I'll work on incorporating custom script into future versions of pipeline\n",
    "    \n",
    "    # QIIME2 import requires a directory containing files names: forward.fastq.gz, reverse.fastq.gz and barcodes.fastq.gz \n",
    "    !ln -s $raw/$read1 $directory/$output/forward.fastq.gz\n",
    "    !ln -s $raw/$read2 $directory/$output/reverse.fastq.gz\n",
    "    \n",
    "    # Gzip the barcodes files (apparently necessary)\n",
    "    !pigz -p 5 $directory/$output/barcodes.fastq\n",
    "\n",
    "    # Remove orphaned reads files (not needed)\n",
    "    !rm $directory/$output/reads?.fastq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Import into QIIME2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime tools import\",\n",
    "        \"--type EMPPairedEndSequences\",\n",
    "        \"--input-path \"+directory+\"/\"+output,\n",
    "        \"--output-path \"+directory+\"/\"+output+\"/\"+name+\".qza\"\n",
    "    ]))\n",
    "    \n",
    "    # This more direct command is broken by the fact QIIME uses multiple dashes in their arguments (is my theory)\n",
    "    #!qiime tools import --type EMPPairedEndSequences --input-path $directory/output --output-path $directory/output/$name.qza\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Demultiplex\n",
    "\n",
    "Notes\n",
    "* The barcode you supply to QIIME is now a concatenation of your forward and reverse barcode.\n",
    "* Your 'forward' barcode is actually the reverse complement of your reverse barcode and the 'reverse' is your forward barcode. The file 'primers.complete.csv' provides this information corresponding to the Buckley Lab 'primer number'\n",
    "* This quirk could be corrected in how different sequencing facilities pre-process the output from the sequencer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SLOW STEP (~ 2 - 4 hrs)\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    metadata = dataset[6]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime demux emp-paired\",\n",
    "        \"--m-barcodes-file \"+directory+\"/\"+metadata,\n",
    "        \"--m-barcodes-column BarcodeSequence\",\n",
    "        \"--p-no-golay-error-correction\",\n",
    "        \"--i-seqs \"+directory+\"/\"+output+\"/\"+name+\".qza\",\n",
    "        \"--o-per-sample-sequences \"+directory+\"/\"+output+\"/\"+name+\".demux\",\n",
    "        \"--o-error-correction-details \"+directory+\"/\"+output+\"/\"+name+\".demux-details.qza\"\n",
    "    ]))\n",
    "    \n",
    "    # This more direct command is broken by the fact QIIME uses multiple dashes in their arguments (is my theory)\n",
    "    #!qiime demux emp-paired --m-barcodes-file $directory/$metadata --m-barcodes-category BarcodeSequence --i-seqs $directory/output/$name.qza --o-per-sample-sequences $directory/output/$name.demux\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Visualize quality scores and determine trimming parameters\n",
    "\n",
    "Based on the graph produced using the following command enter the trim and truncate values. Trim refers to the start of a sequence and truncate the total length (i.e. number of bases to remove from end)\n",
    "\n",
    "Drop output from below command into https://view.qiime2.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime demux summarize\",\n",
    "        \"--i-data \"+directory+\"/\"+output+\"/\"+name+\".demux.qza\",\n",
    "        \"--o-visualization \"+directory+\"/\"+output+\"/\"+name+\".demux.qzv\"\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Trimming parameters | USER INPUT REQUIRED\n",
    "\n",
    "Notes\n",
    "* All trimming parameters must be the same for datasets that will be directly compared to one-another because ASVs are determined by sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## User Input Required\n",
    "\n",
    "## Input your trimming parameters into a python dictionary for all libraries\n",
    "# trim_dict[\"LibraryName1\"] = [trim_forward, truncate_forward, trim_reverse, truncate_reverse]\n",
    "# trim_dict[\"LibraryName2\"] = [trim_forward, truncate_forward, trim_reverse, truncate_reverse]\n",
    "\n",
    "# The example in the Atacam Desert Tutorial trims 13 bp from the start of each read and does not remove any bases from the end of the 150 bp reads:\n",
    "#  --p-trim-left-f 13 \\  \n",
    "#  --p-trim-left-r 13 \\\n",
    "#  --p-trunc-len-f 150 \\\n",
    "#  --p-trunc-len-r 150\n",
    "\n",
    "trim_dict = {}\n",
    "\n",
    "trim_dict[\"xxx\"] = [1,1,1,1] # input values here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Trim, denoise and join (aka 'merge') reads using DADA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SLOW STEP (~ 6 - 8 hrs, IF multithreading is used)\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime dada2 denoise-paired\",\n",
    "        \"--i-demultiplexed-seqs \"+directory+\"/\"+output+\"/\"+name+\".demux.qza\",\n",
    "        \"--o-table \"+directory+\"/\"+output+\"/\"+name+\".table.qza\",\n",
    "        \"--o-representative-sequences \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qza\",\n",
    "        \"--o-denoising-stats \"+directory+\"/\"+output+\"/\"+name+\".denoising-stats.qza\",\n",
    "        \"--p-trim-left-f \"+str(trim_dict[name][0]),\n",
    "        \"--p-trim-left-r \"+str(trim_dict[name][2]),\n",
    "        \"--p-trunc-len-f \"+str(trim_dict[name][1]),\n",
    "        \"--p-trunc-len-r \"+str(trim_dict[name][3]),\n",
    "        \"--p-n-threads\",\n",
    "        str(processors)\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Create summary of ASVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    metadata = dataset[6]\n",
    "    \n",
    "    os.system(' '.join([\n",
    "        \"qiime feature-table summarize\",\n",
    "        \"--i-table \"+directory+\"/\"+output+\"/\"+name+\".table.qza\",\n",
    "        \"--o-visualization \"+directory+\"/\"+output+\"/\"+name+\".table.qzv\",\n",
    "        \"--m-sample-metadata-file \"+directory+\"/\"+metadata\n",
    "    ]))\n",
    "\n",
    "    os.system(' '.join([\n",
    "        \"qiime feature-table tabulate-seqs\",\n",
    "        \"--i-data \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qza\",\n",
    "        \"--o-visualization \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qzv\"\n",
    "    ])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Classify sequences\n",
    "\n",
    "Using SILVA database v128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example of creating Silva Classifier DB  (very slow: use a pre-built one if possible)\n",
    "\n",
    "#### For Silva, remove all the alignment information (unsure of the impact of keeping it) using the following python code:\n",
    "\n",
    "# ```python \n",
    "# import re\n",
    "# from Bio import SeqIO\n",
    "\n",
    "# output = open(\"silva.nr_v128.fasta\", \"w\")\n",
    "\n",
    "# for record in SeqIO.parse(open(\"silva.nr_v128.align\", \"rU\"), \"fasta\") :\n",
    "#     seq = str(record.seq)\n",
    "#     seq = re.sub(\"\\.|-\",\"\",seq)  # Remove \".\" and \"-\"\n",
    "\n",
    "#     output.write(\">\"+record.id+\"\\n\"+seq+\"\\n\")```\n",
    "\n",
    "# #### Import fasta sequence file and taxonomy file as .qza\n",
    "# ```bash\n",
    "# qiime tools import\n",
    "#   --type 'FeatureData[Sequence]'\n",
    "#   --input-path silva.nr_v128.fasta\n",
    "#   --output-path silva.nr_v128.qza\n",
    "\n",
    "# qiime tools import \n",
    "#   --type 'FeatureData[Taxonomy]' \n",
    "#   --source-format HeaderlessTSVTaxonomyFormat \n",
    "#   --input-path silva.nr_v128.tax \n",
    "#   --output-path silva.nr_v128.taxonomy.qza```\n",
    "\n",
    "# #### Run QIIME2 'fit-classifier-naive-bayes'\n",
    "# ```bash\n",
    "# qiime feature-classifier fit-classifier-naive-bayes \n",
    "#   --i-reference-reads silva.nr_v128.qza \n",
    "#   --i-reference-taxonomy silva.nr_v128.taxonomy.qza \n",
    "#   --o-classifier silva.nr_v128.nb.classifier.qza```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Different QIIME2 versions can conflict with previously donwloaded databases. This section might have to be updated.\n",
    "try:\n",
    "    if db == \"GreenGenes\":\n",
    "        classification_db = \"/home/db/GreenGenes/qiime2_13.8.99_515.806_nb.classifier.qza\"\n",
    "    else:\n",
    "        classification_db = \"/home/cassi/databases/silva_v132_classifier100819/silva.nr_v132qiime2.2019.7.classifier.qza\"\n",
    "        \n",
    "except:\n",
    "        classification_db = \"/home/cassi/databases/silva_v132_classifier100819/silva.nr_v132qiime2.2019.7.classifier.qza\"\n",
    "        \n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    metadata = dataset[6]\n",
    "    domain = dataset[7]\n",
    "\n",
    "    # Classify\n",
    "    if domain == 'bacteria':\n",
    "        print(' '.join([\n",
    "            \"qiime feature-classifier classify-sklearn\",\n",
    "            \"--i-classifier\",\n",
    "            classification_db,\n",
    "            \"--i-reads \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qza\",\n",
    "            \"--o-classification \"+directory+\"/\"+output+\"/\"+name+\".taxonomy.qza\",\n",
    "            \"--p-n-jobs\",\n",
    "            str(processors)\n",
    "        ]))\n",
    "\n",
    "    if domain == 'fungi':\n",
    "        os.system(' '.join([\n",
    "            \"qiime feature-classifier classify-sklearn\",\n",
    "            \"--i-classifier /home/db/UNITE/qiime2_unite_ver7.99_20.11.2016_classifier.qza\",\n",
    "            \"--i-reads \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qza\",\n",
    "            \"--o-classification \"+directory+\"/\"+output+\"/\"+name+\".taxonomy.qza\",\n",
    "            \"--p-n-jobs\",\n",
    "            str(processors)\n",
    "        ]))\n",
    "\n",
    "    # Output summary\n",
    "    print(' '.join([\n",
    "        \"qiime metadata tabulate\",\n",
    "        \"--m-input-file \"+directory+\"/\"+output+\"/\"+name+\".taxonomy.qza\",\n",
    "        \"--o-visualization \"+directory+\"/\"+output+\"/\"+name+\".taxonomy-summary.qzv\"\n",
    "    ])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Merge representative sequences for phylogenetic tree\n",
    "\n",
    "If dataset is split into multiple libraries, one tree should be created using all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repseqslist = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    domain = dataset[6]\n",
    "    \n",
    "    if domain != \"fungi\":\n",
    "        repseqslist.append(\"--i-data \" + os.path.join(directory, output, name+\".rep-seqs.qza\"))\n",
    "        \n",
    "    #print(repseqslist)\n",
    "\n",
    "os.system(' '.join([\n",
    "    \"qiime feature-table merge-seqs\",\n",
    "    \" \".join(repseqslist),\n",
    "    \"--o-merged-data \"+treepath+\"/\"+treename+\".rep-seqs-merged.qza\"\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11: Make phylogenetic tree (16S only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    domain = dataset[6]\n",
    "\n",
    "    # Generate alignment with MAFFT\n",
    "    os.system(' '.join([\n",
    "        \"qiime alignment mafft\",\n",
    "        \"--i-sequences \"+treepath+\"/\"+treename+\".rep-seqs-merged.qza\",\n",
    "        \"--o-alignment \"+treepath+\"/\"+treename+\".rep-seqs-merged-aligned.qza\",\n",
    "        \"--p-n-threads\",\n",
    "        str(processors)\n",
    "    ]))\n",
    "    \n",
    "    # Mask hypervariable regions in alignment\n",
    "    os.system(' '.join([\n",
    "        \"qiime alignment mask\",\n",
    "        \"--i-alignment \"+treepath+\"/\"+treename+\".rep-seqs-merged-aligned.qza\",\n",
    "        \"--o-masked-alignment \"+treepath+\"/\"+treename+\".rep-seqs-merged-aligned-masked.qza\",\n",
    "    ]))\n",
    "    \n",
    "    # Generate tree with FastTree\n",
    "    os.system(' '.join([\n",
    "        \"qiime phylogeny fasttree\",\n",
    "        \"--i-alignment \"+treepath+\"/\"+treename+\".rep-seqs-merged-aligned-masked.qza\",\n",
    "        \"--o-tree \"+treepath+\"/\"+treename+\".tree-unrooted.qza\",\n",
    "        \"--p-n-threads\",\n",
    "        str(processors)\n",
    "    ]))\n",
    "    \n",
    "    # Root the tree\n",
    "    os.system(' '.join([\n",
    "        \"qiime phylogeny midpoint-root\",\n",
    "        \"--i-tree \"+treepath+\"/\"+treename+\".tree-unrooted.qza\",\n",
    "        \"--o-rooted-tree \"+treepath+\"/\"+treename+\".tree-rooted.qza\"\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 12: Reformat taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make function to reformat taxonomy file to contain full column information \n",
    "# and factor in the confidence of the taxonomic assignment\n",
    "# modified to include species level identification, no longer differentiates databases, may cause problems for non-Silva db?\n",
    "\n",
    "def format_taxonomy(tax_file, min_support):\n",
    "    output = open(re.sub(\".tsv\",\"-fixed.tsv\",tax_file), \"w\")\n",
    "    \n",
    "    full_rank_length = 7\n",
    "    output.write(\"\\t\".join([\"OTU\",\"Domain\",\"Phylum\",\"Class\",\"Order\",\"Family\",\"Genus\",\"Species\"])+\"\\n\")\n",
    "\n",
    "    with open(tax_file, \"r\") as f:\n",
    "        next(f)\n",
    "\n",
    "        for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "\n",
    "                read_id = line[0]\n",
    "                tax_string = line[1]\n",
    "\n",
    "                # Split full rank into ranks\n",
    "                full_rank = tax_string.split(\";\")\n",
    "\n",
    "                ## Identify the lowest classified taxonomic rank\n",
    "                # Account for cases when a taxonomic rank contains an empty space (common in GreenGenes output)\n",
    "                last_classified = full_rank[len(full_rank)-1]            \n",
    "\n",
    "                count = 1\n",
    "                while last_classified == \" \":\n",
    "                    last_classified = full_rank[len(full_rank)-count]\n",
    "                    count = count + 1\n",
    "\n",
    "                # Annotate the last classified as 'putative' if it does not meet the minimum support criteria\n",
    "                if float(line[2]) < float(min_support):\n",
    "                        full_rank[full_rank.index(last_classified)] = \"putative \"+last_classified\n",
    "                        last_classified = \"putative \"+last_classified\n",
    "\n",
    "                #print(last_classified)\n",
    "                # Chloroplast, didn't change because confidence > 0.9\n",
    "\n",
    "                # Add in columns containing unclassified taxonomic information\n",
    "                for n in range(full_rank.index(last_classified)+1, full_rank_length, 1):               \n",
    "                    try:\n",
    "                        full_rank[n] = \"unclassified \"+last_classified\n",
    "                    except:\n",
    "                        full_rank.append(\"unclassified \"+last_classified)\n",
    "\n",
    "                # Write taxonomy to file\n",
    "                output.write(read_id+\"\\t\"+'\\t'.join(full_rank)+\"\\n\")\n",
    "                \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 13: Export from QIIME2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset[0]\n",
    "    directory = dataset[1]\n",
    "    output = dataset[2]\n",
    "    metadata = dataset[6]\n",
    "\n",
    "    # Final output names\n",
    "    fasta_file = directory+\"/\"+output+\"/final/\"+name+\".rep-seqs-final.fasta\"\n",
    "    tax_file = directory+\"/\"+output+\"/final/\"+name+\".taxonomy-final.tsv\"\n",
    "    count_table = directory+\"/\"+output+\"/final/\"+name+\".counts-final.biom\"\n",
    "    tax_tree = treepath+\"/\"+treename+\".tree-final.nwk\"\n",
    "    \n",
    "    # Make final data directories\n",
    "    if not os.path.isdir(os.path.join(directory, output, \"final\")):\n",
    "        !mkdir $directory/$output/final\n",
    "\n",
    "    with open(os.path.join(directory, output, name+\"repseqslist\"), \"w\") as outfile:\n",
    "        for i in repseqslist:\n",
    "            outfile.write(i+\"\\n\")\n",
    "\n",
    "    # Export classifications\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        \"--input-path \"+directory+\"/\"+output+\"/\"+name+\".taxonomy.qza\",\n",
    "        \"--output-path \"+directory+\"/\"+output+\"/\"\n",
    "    ]))\n",
    "    \n",
    "    # Reformat classifications to meet phyloseq format   \n",
    "    format_taxonomy(directory+\"/\"+output+\"/taxonomy.tsv\", min_support)\n",
    "\n",
    "    # Export ESV table\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        \"--input-path \"+directory+\"/\"+output+\"/\"+name+\".table.qza\",\n",
    "        \"--output-path \"+directory+\"/\"+output+\"/\"\n",
    "    ]))\n",
    "\n",
    "    # Export ESV sequences\n",
    "    os.system(' '.join([\n",
    "        \"qiime tools export\",\n",
    "        \"--input-path \"+directory+\"/\"+output+\"/\"+name+\".rep-seqs.qza\",\n",
    "        \"--output-path \"+directory+\"/\"+output+\"/\"\n",
    "    ]))\n",
    "    \n",
    "    # Rename exported files\n",
    "    %mv $directory/$output/dna-sequences.fasta $fasta_file\n",
    "    %mv $directory/$output/feature-table.biom $count_table\n",
    "    %mv $directory/$output/taxonomy-fixed.tsv $tax_file\n",
    "    \n",
    "# Export tree\n",
    "os.system(' '.join([\n",
    "    \"qiime tools export\",\n",
    "    \"--input-path \"+treepath+\"/\"+treename+\".tree-rooted.qza\",\n",
    "    \"--output-path \"+treepath+\"/\"\n",
    "]))\n",
    "\n",
    "# Rename tree file\n",
    "%mv $treepath/$treename+.tree.nwk $tree_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
